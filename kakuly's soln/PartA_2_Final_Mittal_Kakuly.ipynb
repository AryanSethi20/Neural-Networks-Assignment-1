{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c7e82aadc4d77a8b23f7f880449f9e3",
     "grade": false,
     "grade_id": "a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Question A2 (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4ac2a-d56e-4151-8e0a-4a833cbc643e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "26b4ac2a-d56e-4151-8e0a-4a833cbc643e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb28aa752ce5540f5b18d10694b52ea9",
     "grade": false,
     "grade_id": "a22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### In this question, we will determine the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 5-fold cross-validation on training partition to perform hyperparameter selection. You will have to reconsider the scaling of the dataset during the 5-fold cross validation.\n",
    "\n",
    "* note: some cells are non-editable and cannot be filled, but leave them untouched. Fill up only cells which are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aceec82011f43733c0551ca196f1b16c",
     "grade": false,
     "grade_id": "a2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch sizes {128, 256, 512, 1024}. Next, create a table of time taken to train the network on the last epoch against different batch sizes. Finally, select the optimal batch size and state a reason for your selection.\n",
    "\n",
    "This might take a while to run, so plan your time carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0edc610-21e6-4cc7-9603-59318b961990",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b0edc610-21e6-4cc7-9603-59318b961990",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "909acb3c7ff3883eb5381eb586615d3b",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e12861-4713-4914-9f4b-8a7381708243",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e8e12861-4713-4914-9f4b-8a7381708243",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed97d9f30da032a5e349047c614efec1",
     "grade": false,
     "grade_id": "a2_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2. To reduce repeated code, place your\n",
    "\n",
    "- network (MLP defined in QA1)\n",
    "- torch datasets (CustomDataset defined in QA1)\n",
    "- loss function (loss_fn defined in QA1)\n",
    "\n",
    "in a separate file called **common_utils.py**\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a1a982-de85-46de-b890-3b81f79f5887",
   "metadata": {
    "deletable": false,
    "id": "37a1a982-de85-46de-b890-3b81f79f5887",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9db3ca972642b1447dba3ebd5f2db24b",
     "grade": false,
     "grade_id": "import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from common_utils import MLP, intialise_loaders, train_loop, test_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa562e7-23c3-4920-ae63-4563bf30e39d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5aa562e7-23c3-4920-ae63-4563bf30e39d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6b33318200b4bc38d431576963edb1",
     "grade": true,
     "grade_id": "correct_import",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82ea67d6-1eb4-428d-9407-9d988e927ff6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "82ea67d6-1eb4-428d-9407-9d988e927ff6",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c738d3b4888de90dda8c532036bc5fe5",
     "grade": false,
     "grade_id": "a2_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "3. Define different folds for different batch sizes to get a dictionary of training and validation datasets. Preprocess your datasets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ef6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import split_dataset, preprocess_dataset\n",
    "\n",
    "def preprocess(df):\n",
    "    # Preporocessing\n",
    "    f_columns_to_drop = ['label', 'filename'] #These columns are dropped to get only the features\n",
    "    X_train_notscaled, y_train, X_test_notscaled, y_test = split_dataset(df, columns_to_drop = f_columns_to_drop, test_size = 0.3, random_state = 0)\n",
    "    return X_train_notscaled, y_train, X_test_notscaled, y_test\n",
    "\n",
    "\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "df['label'].value_counts()\n",
    "X_train_notscaled, y_train, X_test_notscaled, y_test = preprocess(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deab683a-2c9e-4e62-823a-e8b4a186bda8",
   "metadata": {
    "deletable": false,
    "id": "deab683a-2c9e-4e62-823a-e8b4a186bda8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d02dac62baa528c191eb4f47b2495406",
     "grade": false,
     "grade_id": "dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_utils import preprocess_dataset\n",
    "def generate_cv_folds_for_batch_sizes(parameters, X_train, y_train):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    X_train_scaled_dict(dict) where X_train_scaled_dict[batch_size] is a list of the preprocessed training matrix for the different folds.\n",
    "    X_val_scaled_dict(dict) where X_val_scaled_dict[batch_size] is a list of the processed validation matrix for the different folds.\n",
    "    y_train_dict(dict) where y_train_dict[batch_size] is a list of labels for the different folds\n",
    "    y_val_dict(dict) where y_val_dict[batch_size] is a list of labels for the different folds\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict = {}, {}, {}, {}\n",
    "    for batch in parameters:\n",
    "        cv = KFold(n_splits=no_folds, shuffle=True, random_state=1)\n",
    "        X_train_scaled_dict[batch] = []\n",
    "        X_val_scaled_dict[batch] = []\n",
    "        y_train_dict[batch] = []\n",
    "        y_val_dict[batch] = []\n",
    "        for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "            x_train, y_train_1  = X_train[train_idx], y_train[train_idx]\n",
    "            x_val, y_val = X_train[val_idx], y_train[val_idx]\n",
    "            X_train_scaled, X_val_scaled = preprocess_dataset(x_train, x_val)\n",
    "            \n",
    "            X_train_scaled_dict[batch].append(X_train_scaled)\n",
    "            X_val_scaled_dict[batch].append(X_val_scaled)\n",
    "            y_train_dict[batch].append(y_train_1)\n",
    "            y_val_dict[batch].append(y_val)\n",
    "        \n",
    "            \n",
    "    return X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict\n",
    "\n",
    "no_folds = 5\n",
    "batch_sizes = [128, 256, 512, 1024]\n",
    "X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict = generate_cv_folds_for_batch_sizes(batch_sizes, X_train_notscaled.to_numpy(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ca332-9676-42bd-9801-0f5f4157a777",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "235ca332-9676-42bd-9801-0f5f4157a777",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae5f281cd84f4d36f81f2ae126cf915",
     "grade": true,
     "grade_id": "correct_dataset",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df744af-f485-4871-9e0a-70fd41d1df4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8df744af-f485-4871-9e0a-70fd41d1df4d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dcf6be1ad49306172e6f27243e613f2",
     "grade": true,
     "grade_id": "correct_dataset2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "064d68c9708b5e3f1e2463001b6d78b4",
     "grade": false,
     "grade_id": "a2_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "4. Perform hyperparameter tuning for the different batch sizes with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f58a02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import EarlyStopper\n",
    "model_no_features = 77\n",
    "model_no_hidden = 128\n",
    "model_no_labels = 1\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "es_patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3107ebe9-d121-4510-9782-2a62d32258d0",
   "metadata": {
    "deletable": false,
    "id": "3107ebe9-d121-4510-9782-2a62d32258d0",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9665887943f38ae7bed6c1d8351903b",
     "grade": true,
     "grade_id": "hyperparameter_tuning",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "----------------------------------\n",
      "\n",
      "\n",
      "Experiment 1\n",
      "loss: 0.691208  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.687314 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.682745 \n",
      "\n",
      "Epoch 1: Train_accuracy: 53.96%, Train_loss: 0.687314, Test_accuracy: 57.29%, Test_loss: 0.682745\n",
      "\n",
      "\n",
      "loss: 0.684178  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.673916 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.669261 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.93%, Train_loss: 0.673916, Test_accuracy: 58.35%, Test_loss: 0.669261\n",
      "\n",
      "\n",
      "loss: 0.617060  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.657224 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.662261 \n",
      "\n",
      "Epoch 3: Train_accuracy: 61.59%, Train_loss: 0.657224, Test_accuracy: 60.84%, Test_loss: 0.662261\n",
      "\n",
      "\n",
      "loss: 0.620799  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.646818 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.651992 \n",
      "\n",
      "Epoch 4: Train_accuracy: 62.21%, Train_loss: 0.646818, Test_accuracy: 61.26%, Test_loss: 0.651992\n",
      "\n",
      "\n",
      "loss: 0.599134  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.629878 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.644629 \n",
      "\n",
      "Epoch 5: Train_accuracy: 65.09%, Train_loss: 0.629878, Test_accuracy: 62.14%, Test_loss: 0.644629\n",
      "\n",
      "\n",
      "loss: 0.616230  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.608105 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.664707 \n",
      "\n",
      "Epoch 6: Train_accuracy: 67.04%, Train_loss: 0.608105, Test_accuracy: 61.08%, Test_loss: 0.664707\n",
      "\n",
      "\n",
      "loss: 0.594512  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.599959 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.630507 \n",
      "\n",
      "Epoch 7: Train_accuracy: 67.34%, Train_loss: 0.599959, Test_accuracy: 64.45%, Test_loss: 0.630507\n",
      "\n",
      "\n",
      "loss: 0.562910  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.584275 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.629967 \n",
      "\n",
      "Epoch 8: Train_accuracy: 69.10%, Train_loss: 0.584275, Test_accuracy: 63.86%, Test_loss: 0.629967\n",
      "\n",
      "\n",
      "loss: 0.579729  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.563974 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.634468 \n",
      "\n",
      "Epoch 9: Train_accuracy: 70.32%, Train_loss: 0.563974, Test_accuracy: 65.52%, Test_loss: 0.634468\n",
      "\n",
      "\n",
      "loss: 0.536661  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.546788 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.637619 \n",
      "\n",
      "Epoch 10: Train_accuracy: 71.71%, Train_loss: 0.546788, Test_accuracy: 66.17%, Test_loss: 0.637619\n",
      "\n",
      "\n",
      "loss: 0.486244  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.527587 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.652259 \n",
      "\n",
      "Epoch 11: Train_accuracy: 73.62%, Train_loss: 0.527587, Test_accuracy: 66.00%, Test_loss: 0.652259\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 2\n",
      "loss: 0.693495  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.687352 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.681063 \n",
      "\n",
      "Epoch 1: Train_accuracy: 54.75%, Train_loss: 0.687352, Test_accuracy: 56.46%, Test_loss: 0.681063\n",
      "\n",
      "\n",
      "loss: 0.680913  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.673169 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.667932 \n",
      "\n",
      "Epoch 2: Train_accuracy: 58.23%, Train_loss: 0.673169, Test_accuracy: 59.48%, Test_loss: 0.667932\n",
      "\n",
      "\n",
      "loss: 0.651355  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.659953 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.659226 \n",
      "\n",
      "Epoch 3: Train_accuracy: 61.09%, Train_loss: 0.659953, Test_accuracy: 59.42%, Test_loss: 0.659226\n",
      "\n",
      "\n",
      "loss: 0.679667  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.646575 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.649953 \n",
      "\n",
      "Epoch 4: Train_accuracy: 62.58%, Train_loss: 0.646575, Test_accuracy: 61.91%, Test_loss: 0.649953\n",
      "\n",
      "\n",
      "loss: 0.647881  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.632669 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.632224 \n",
      "\n",
      "Epoch 5: Train_accuracy: 64.30%, Train_loss: 0.632669, Test_accuracy: 64.75%, Test_loss: 0.632224\n",
      "\n",
      "\n",
      "loss: 0.573604  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.615429 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.627998 \n",
      "\n",
      "Epoch 6: Train_accuracy: 65.68%, Train_loss: 0.615429, Test_accuracy: 64.22%, Test_loss: 0.627998\n",
      "\n",
      "\n",
      "loss: 0.634336  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.593708 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.626561 \n",
      "\n",
      "Epoch 7: Train_accuracy: 67.84%, Train_loss: 0.593708, Test_accuracy: 66.41%, Test_loss: 0.626561\n",
      "\n",
      "\n",
      "loss: 0.587645  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.582095 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.601392 \n",
      "\n",
      "Epoch 8: Train_accuracy: 69.07%, Train_loss: 0.582095, Test_accuracy: 66.11%, Test_loss: 0.601392\n",
      "\n",
      "\n",
      "loss: 0.560026  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.569322 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.607535 \n",
      "\n",
      "Epoch 9: Train_accuracy: 69.80%, Train_loss: 0.569322, Test_accuracy: 66.94%, Test_loss: 0.607535\n",
      "\n",
      "\n",
      "loss: 0.501976  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.549225 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.602724 \n",
      "\n",
      "Epoch 10: Train_accuracy: 72.06%, Train_loss: 0.549225, Test_accuracy: 68.01%, Test_loss: 0.602724\n",
      "\n",
      "\n",
      "loss: 0.504129  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.531033 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.589316 \n",
      "\n",
      "Epoch 11: Train_accuracy: 73.46%, Train_loss: 0.531033, Test_accuracy: 66.77%, Test_loss: 0.589316\n",
      "\n",
      "\n",
      "loss: 0.481857  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.520384 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.610185 \n",
      "\n",
      "Epoch 12: Train_accuracy: 74.11%, Train_loss: 0.520384, Test_accuracy: 65.94%, Test_loss: 0.610185\n",
      "\n",
      "\n",
      "loss: 0.432883  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.506075 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.618049 \n",
      "\n",
      "Epoch 13: Train_accuracy: 75.10%, Train_loss: 0.506075, Test_accuracy: 68.07%, Test_loss: 0.618049\n",
      "\n",
      "\n",
      "loss: 0.427999  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.485892 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.610625 \n",
      "\n",
      "Epoch 14: Train_accuracy: 75.94%, Train_loss: 0.485892, Test_accuracy: 69.85%, Test_loss: 0.610625\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 3\n",
      "loss: 0.694571  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.687633 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.682919 \n",
      "\n",
      "Epoch 1: Train_accuracy: 54.69%, Train_loss: 0.687633, Test_accuracy: 56.40%, Test_loss: 0.682919\n",
      "\n",
      "\n",
      "loss: 0.666843  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.672029 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.678642 \n",
      "\n",
      "Epoch 2: Train_accuracy: 58.47%, Train_loss: 0.672029, Test_accuracy: 57.05%, Test_loss: 0.678642\n",
      "\n",
      "\n",
      "loss: 0.657223  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.656472 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.664281 \n",
      "\n",
      "Epoch 3: Train_accuracy: 60.87%, Train_loss: 0.656472, Test_accuracy: 58.41%, Test_loss: 0.664281\n",
      "\n",
      "\n",
      "loss: 0.638676  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.640025 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.662762 \n",
      "\n",
      "Epoch 4: Train_accuracy: 63.12%, Train_loss: 0.640025, Test_accuracy: 60.37%, Test_loss: 0.662762\n",
      "\n",
      "\n",
      "loss: 0.624678  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.628534 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.651548 \n",
      "\n",
      "Epoch 5: Train_accuracy: 64.20%, Train_loss: 0.628534, Test_accuracy: 61.91%, Test_loss: 0.651548\n",
      "\n",
      "\n",
      "loss: 0.593707  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.611313 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.645716 \n",
      "\n",
      "Epoch 6: Train_accuracy: 66.66%, Train_loss: 0.611313, Test_accuracy: 62.56%, Test_loss: 0.645716\n",
      "\n",
      "\n",
      "loss: 0.605331  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.594492 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.641129 \n",
      "\n",
      "Epoch 7: Train_accuracy: 67.95%, Train_loss: 0.594492, Test_accuracy: 62.56%, Test_loss: 0.641129\n",
      "\n",
      "\n",
      "loss: 0.517745  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.584215 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.627711 \n",
      "\n",
      "Epoch 8: Train_accuracy: 69.43%, Train_loss: 0.584215, Test_accuracy: 64.69%, Test_loss: 0.627711\n",
      "\n",
      "\n",
      "loss: 0.547821  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.561122 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.641429 \n",
      "\n",
      "Epoch 9: Train_accuracy: 71.44%, Train_loss: 0.561122, Test_accuracy: 64.69%, Test_loss: 0.641429\n",
      "\n",
      "\n",
      "loss: 0.532209  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.544443 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.617268 \n",
      "\n",
      "Epoch 10: Train_accuracy: 72.48%, Train_loss: 0.544443, Test_accuracy: 67.71%, Test_loss: 0.617268\n",
      "\n",
      "\n",
      "loss: 0.599777  [  128/ 6751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.533615 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.639261 \n",
      "\n",
      "Epoch 11: Train_accuracy: 73.50%, Train_loss: 0.533615, Test_accuracy: 66.17%, Test_loss: 0.639261\n",
      "\n",
      "\n",
      "loss: 0.493420  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.513988 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.627973 \n",
      "\n",
      "Epoch 12: Train_accuracy: 74.46%, Train_loss: 0.513988, Test_accuracy: 68.42%, Test_loss: 0.627973\n",
      "\n",
      "\n",
      "loss: 0.479303  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.497024 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.624658 \n",
      "\n",
      "Epoch 13: Train_accuracy: 75.83%, Train_loss: 0.497024, Test_accuracy: 67.48%, Test_loss: 0.624658\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 4\n",
      "loss: 0.695845  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.685868 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.680395 \n",
      "\n",
      "Epoch 1: Train_accuracy: 54.85%, Train_loss: 0.685868, Test_accuracy: 57.52%, Test_loss: 0.680395\n",
      "\n",
      "\n",
      "loss: 0.660630  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.670421 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.669827 \n",
      "\n",
      "Epoch 2: Train_accuracy: 58.88%, Train_loss: 0.670421, Test_accuracy: 60.55%, Test_loss: 0.669827\n",
      "\n",
      "\n",
      "loss: 0.675196  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.654741 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.678867 \n",
      "\n",
      "Epoch 3: Train_accuracy: 60.81%, Train_loss: 0.654741, Test_accuracy: 59.42%, Test_loss: 0.678867\n",
      "\n",
      "\n",
      "loss: 0.637764  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.640481 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.658100 \n",
      "\n",
      "Epoch 4: Train_accuracy: 63.15%, Train_loss: 0.640481, Test_accuracy: 61.20%, Test_loss: 0.658100\n",
      "\n",
      "\n",
      "loss: 0.631297  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.628764 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.650932 \n",
      "\n",
      "Epoch 5: Train_accuracy: 64.55%, Train_loss: 0.628764, Test_accuracy: 62.20%, Test_loss: 0.650932\n",
      "\n",
      "\n",
      "loss: 0.631175  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.603497 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.648623 \n",
      "\n",
      "Epoch 6: Train_accuracy: 66.70%, Train_loss: 0.603497, Test_accuracy: 61.67%, Test_loss: 0.648623\n",
      "\n",
      "\n",
      "loss: 0.560328  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.594587 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.645054 \n",
      "\n",
      "Epoch 7: Train_accuracy: 67.99%, Train_loss: 0.594587, Test_accuracy: 62.32%, Test_loss: 0.645054\n",
      "\n",
      "\n",
      "loss: 0.615147  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.577565 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.651098 \n",
      "\n",
      "Epoch 8: Train_accuracy: 69.90%, Train_loss: 0.577565, Test_accuracy: 64.28%, Test_loss: 0.651098\n",
      "\n",
      "\n",
      "loss: 0.605232  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.563251 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.616538 \n",
      "\n",
      "Epoch 9: Train_accuracy: 70.45%, Train_loss: 0.563251, Test_accuracy: 66.47%, Test_loss: 0.616538\n",
      "\n",
      "\n",
      "loss: 0.511550  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.548880 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.640934 \n",
      "\n",
      "Epoch 10: Train_accuracy: 71.47%, Train_loss: 0.548880, Test_accuracy: 66.11%, Test_loss: 0.640934\n",
      "\n",
      "\n",
      "loss: 0.567316  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.533811 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.630640 \n",
      "\n",
      "Epoch 11: Train_accuracy: 72.36%, Train_loss: 0.533811, Test_accuracy: 66.00%, Test_loss: 0.630640\n",
      "\n",
      "\n",
      "loss: 0.458577  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.517599 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.604839 \n",
      "\n",
      "Epoch 12: Train_accuracy: 74.15%, Train_loss: 0.517599, Test_accuracy: 66.88%, Test_loss: 0.604839\n",
      "\n",
      "\n",
      "loss: 0.488018  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.505885 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.619139 \n",
      "\n",
      "Epoch 13: Train_accuracy: 74.95%, Train_loss: 0.505885, Test_accuracy: 67.77%, Test_loss: 0.619139\n",
      "\n",
      "\n",
      "loss: 0.499564  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.484919 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.660093 \n",
      "\n",
      "Epoch 14: Train_accuracy: 75.53%, Train_loss: 0.484919, Test_accuracy: 66.53%, Test_loss: 0.660093\n",
      "\n",
      "\n",
      "loss: 0.506497  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.482574 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.605077 \n",
      "\n",
      "Epoch 15: Train_accuracy: 76.58%, Train_loss: 0.482574, Test_accuracy: 68.01%, Test_loss: 0.605077\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 5\n",
      "loss: 0.693558  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.688017 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.679785 \n",
      "\n",
      "Epoch 1: Train_accuracy: 53.64%, Train_loss: 0.688017, Test_accuracy: 56.43%, Test_loss: 0.679785\n",
      "\n",
      "\n",
      "loss: 0.698606  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.671611 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.677790 \n",
      "\n",
      "Epoch 2: Train_accuracy: 58.65%, Train_loss: 0.671611, Test_accuracy: 57.50%, Test_loss: 0.677790\n",
      "\n",
      "\n",
      "loss: 0.644111  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.656994 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.662488 \n",
      "\n",
      "Epoch 3: Train_accuracy: 60.68%, Train_loss: 0.656994, Test_accuracy: 59.10%, Test_loss: 0.662488\n",
      "\n",
      "\n",
      "loss: 0.635363  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.639974 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.652605 \n",
      "\n",
      "Epoch 4: Train_accuracy: 62.72%, Train_loss: 0.639974, Test_accuracy: 60.76%, Test_loss: 0.652605\n",
      "\n",
      "\n",
      "loss: 0.684143  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.629445 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.651564 \n",
      "\n",
      "Epoch 5: Train_accuracy: 64.74%, Train_loss: 0.629445, Test_accuracy: 61.00%, Test_loss: 0.651564\n",
      "\n",
      "\n",
      "loss: 0.584965  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.615173 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.638433 \n",
      "\n",
      "Epoch 6: Train_accuracy: 65.23%, Train_loss: 0.615173, Test_accuracy: 63.13%, Test_loss: 0.638433\n",
      "\n",
      "\n",
      "loss: 0.641758  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.596510 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.654759 \n",
      "\n",
      "Epoch 7: Train_accuracy: 68.07%, Train_loss: 0.596510, Test_accuracy: 61.41%, Test_loss: 0.654759\n",
      "\n",
      "\n",
      "loss: 0.531956  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.587513 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.636490 \n",
      "\n",
      "Epoch 8: Train_accuracy: 68.72%, Train_loss: 0.587513, Test_accuracy: 62.48%, Test_loss: 0.636490\n",
      "\n",
      "\n",
      "loss: 0.560402  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.561773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.629973 \n",
      "\n",
      "Epoch 9: Train_accuracy: 70.53%, Train_loss: 0.561773, Test_accuracy: 65.56%, Test_loss: 0.629973\n",
      "\n",
      "\n",
      "loss: 0.701856  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.552467 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.623614 \n",
      "\n",
      "Epoch 10: Train_accuracy: 71.37%, Train_loss: 0.552467, Test_accuracy: 66.57%, Test_loss: 0.623614\n",
      "\n",
      "\n",
      "loss: 0.500730  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.535703 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.628922 \n",
      "\n",
      "Epoch 11: Train_accuracy: 72.84%, Train_loss: 0.535703, Test_accuracy: 64.26%, Test_loss: 0.628922\n",
      "\n",
      "\n",
      "loss: 0.464447  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.521911 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.619219 \n",
      "\n",
      "Epoch 12: Train_accuracy: 73.19%, Train_loss: 0.521911, Test_accuracy: 65.62%, Test_loss: 0.619219\n",
      "\n",
      "\n",
      "loss: 0.512015  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.509736 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.627633 \n",
      "\n",
      "Epoch 13: Train_accuracy: 74.29%, Train_loss: 0.509736, Test_accuracy: 65.20%, Test_loss: 0.627633\n",
      "\n",
      "\n",
      "loss: 0.502444  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.501698 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.612789 \n",
      "\n",
      "Epoch 14: Train_accuracy: 75.04%, Train_loss: 0.501698, Test_accuracy: 67.22%, Test_loss: 0.612789\n",
      "\n",
      "\n",
      "loss: 0.446547  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.487904 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.610373 \n",
      "\n",
      "Epoch 15: Train_accuracy: 75.95%, Train_loss: 0.487904, Test_accuracy: 67.52%, Test_loss: 0.610373\n",
      "\n",
      "\n",
      "loss: 0.446756  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.477145 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.609976 \n",
      "\n",
      "Epoch 16: Train_accuracy: 76.82%, Train_loss: 0.477145, Test_accuracy: 69.29%, Test_loss: 0.609976\n",
      "\n",
      "\n",
      "loss: 0.405588  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.453399 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.626347 \n",
      "\n",
      "Epoch 17: Train_accuracy: 78.69%, Train_loss: 0.453399, Test_accuracy: 68.64%, Test_loss: 0.626347\n",
      "\n",
      "\n",
      "loss: 0.531473  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.457453 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.651751 \n",
      "\n",
      "Epoch 18: Train_accuracy: 78.36%, Train_loss: 0.457453, Test_accuracy: 67.04%, Test_loss: 0.651751\n",
      "\n",
      "\n",
      "loss: 0.454404  [  128/ 6752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.440313 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.604020 \n",
      "\n",
      "Epoch 19: Train_accuracy: 79.27%, Train_loss: 0.440313, Test_accuracy: 67.87%, Test_loss: 0.604020\n",
      "\n",
      "\n",
      "loss: 0.442150  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.422427 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.645715 \n",
      "\n",
      "Epoch 20: Train_accuracy: 79.99%, Train_loss: 0.422427, Test_accuracy: 66.57%, Test_loss: 0.645715\n",
      "\n",
      "\n",
      "loss: 0.365176  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.413337 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.620327 \n",
      "\n",
      "Epoch 21: Train_accuracy: 80.41%, Train_loss: 0.413337, Test_accuracy: 69.35%, Test_loss: 0.620327\n",
      "\n",
      "\n",
      "loss: 0.343490  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.411131 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.656935 \n",
      "\n",
      "Epoch 22: Train_accuracy: 81.58%, Train_loss: 0.411131, Test_accuracy: 68.52%, Test_loss: 0.656935\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "256\n",
      "----------------------------------\n",
      "\n",
      "\n",
      "Experiment 1\n",
      "loss: 0.692264  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.689327 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.685007 \n",
      "\n",
      "Epoch 1: Train_accuracy: 53.99%, Train_loss: 0.689327, Test_accuracy: 55.57%, Test_loss: 0.685007\n",
      "\n",
      "\n",
      "loss: 0.695010  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.679261 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.674968 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.21%, Train_loss: 0.679261, Test_accuracy: 58.12%, Test_loss: 0.674968\n",
      "\n",
      "\n",
      "loss: 0.693833  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.667190 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.669225 \n",
      "\n",
      "Epoch 3: Train_accuracy: 59.13%, Train_loss: 0.667190, Test_accuracy: 59.83%, Test_loss: 0.669225\n",
      "\n",
      "\n",
      "loss: 0.657269  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.653951 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.671456 \n",
      "\n",
      "Epoch 4: Train_accuracy: 61.56%, Train_loss: 0.653951, Test_accuracy: 59.24%, Test_loss: 0.671456\n",
      "\n",
      "\n",
      "loss: 0.641134  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.640271 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.656189 \n",
      "\n",
      "Epoch 5: Train_accuracy: 63.50%, Train_loss: 0.640271, Test_accuracy: 60.43%, Test_loss: 0.656189\n",
      "\n",
      "\n",
      "loss: 0.636337  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.623080 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.652458 \n",
      "\n",
      "Epoch 6: Train_accuracy: 64.66%, Train_loss: 0.623080, Test_accuracy: 61.73%, Test_loss: 0.652458\n",
      "\n",
      "\n",
      "loss: 0.598994  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.612567 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.638230 \n",
      "\n",
      "Epoch 7: Train_accuracy: 66.58%, Train_loss: 0.612567, Test_accuracy: 63.80%, Test_loss: 0.638230\n",
      "\n",
      "\n",
      "loss: 0.601223  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.597382 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.653922 \n",
      "\n",
      "Epoch 8: Train_accuracy: 68.12%, Train_loss: 0.597382, Test_accuracy: 62.68%, Test_loss: 0.653922\n",
      "\n",
      "\n",
      "loss: 0.580858  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.581382 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.646573 \n",
      "\n",
      "Epoch 9: Train_accuracy: 69.72%, Train_loss: 0.581382, Test_accuracy: 63.27%, Test_loss: 0.646573\n",
      "\n",
      "\n",
      "loss: 0.591382  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.573954 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.629888 \n",
      "\n",
      "Epoch 10: Train_accuracy: 69.95%, Train_loss: 0.573954, Test_accuracy: 65.58%, Test_loss: 0.629888\n",
      "\n",
      "\n",
      "loss: 0.532927  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.550339 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.657335 \n",
      "\n",
      "Epoch 11: Train_accuracy: 71.75%, Train_loss: 0.550339, Test_accuracy: 62.97%, Test_loss: 0.657335\n",
      "\n",
      "\n",
      "loss: 0.573725  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.543446 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.641372 \n",
      "\n",
      "Epoch 12: Train_accuracy: 72.58%, Train_loss: 0.543446, Test_accuracy: 64.87%, Test_loss: 0.641372\n",
      "\n",
      "\n",
      "loss: 0.534776  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.528672 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.631151 \n",
      "\n",
      "Epoch 13: Train_accuracy: 73.74%, Train_loss: 0.528672, Test_accuracy: 64.99%, Test_loss: 0.631151\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 2\n",
      "loss: 0.696782  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.687790 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 0.684378 \n",
      "\n",
      "Epoch 1: Train_accuracy: 53.83%, Train_loss: 0.687790, Test_accuracy: 54.80%, Test_loss: 0.684378\n",
      "\n",
      "\n",
      "loss: 0.685425  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.678550 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.676970 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.07%, Train_loss: 0.678550, Test_accuracy: 57.64%, Test_loss: 0.676970\n",
      "\n",
      "\n",
      "loss: 0.677107  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.667373 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.663545 \n",
      "\n",
      "Epoch 3: Train_accuracy: 59.29%, Train_loss: 0.667373, Test_accuracy: 59.24%, Test_loss: 0.663545\n",
      "\n",
      "\n",
      "loss: 0.658743  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.657008 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.651788 \n",
      "\n",
      "Epoch 4: Train_accuracy: 60.52%, Train_loss: 0.657008, Test_accuracy: 60.31%, Test_loss: 0.651788\n",
      "\n",
      "\n",
      "loss: 0.663748  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.643443 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.651083 \n",
      "\n",
      "Epoch 5: Train_accuracy: 62.41%, Train_loss: 0.643443, Test_accuracy: 60.31%, Test_loss: 0.651083\n",
      "\n",
      "\n",
      "loss: 0.607994  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.628214 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.637946 \n",
      "\n",
      "Epoch 6: Train_accuracy: 64.73%, Train_loss: 0.628214, Test_accuracy: 61.37%, Test_loss: 0.637946\n",
      "\n",
      "\n",
      "loss: 0.591444  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.614034 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.639493 \n",
      "\n",
      "Epoch 7: Train_accuracy: 66.09%, Train_loss: 0.614034, Test_accuracy: 62.32%, Test_loss: 0.639493\n",
      "\n",
      "\n",
      "loss: 0.606349  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.603758 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.629837 \n",
      "\n",
      "Epoch 8: Train_accuracy: 66.95%, Train_loss: 0.603758, Test_accuracy: 63.80%, Test_loss: 0.629837\n",
      "\n",
      "\n",
      "loss: 0.570287  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.591680 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.631954 \n",
      "\n",
      "Epoch 9: Train_accuracy: 67.78%, Train_loss: 0.591680, Test_accuracy: 64.69%, Test_loss: 0.631954\n",
      "\n",
      "\n",
      "loss: 0.586132  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.581154 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.612433 \n",
      "\n",
      "Epoch 10: Train_accuracy: 68.92%, Train_loss: 0.581154, Test_accuracy: 66.47%, Test_loss: 0.612433\n",
      "\n",
      "\n",
      "loss: 0.609947  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.566841 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.626002 \n",
      "\n",
      "Epoch 11: Train_accuracy: 69.92%, Train_loss: 0.566841, Test_accuracy: 66.59%, Test_loss: 0.626002\n",
      "\n",
      "\n",
      "loss: 0.546802  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.554060 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.612110 \n",
      "\n",
      "Epoch 12: Train_accuracy: 70.58%, Train_loss: 0.554060, Test_accuracy: 67.95%, Test_loss: 0.612110\n",
      "\n",
      "\n",
      "loss: 0.569162  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.545179 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.607929 \n",
      "\n",
      "Epoch 13: Train_accuracy: 72.00%, Train_loss: 0.545179, Test_accuracy: 68.25%, Test_loss: 0.607929\n",
      "\n",
      "\n",
      "loss: 0.520283  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.537735 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.593476 \n",
      "\n",
      "Epoch 14: Train_accuracy: 73.31%, Train_loss: 0.537735, Test_accuracy: 67.89%, Test_loss: 0.593476\n",
      "\n",
      "\n",
      "loss: 0.524094  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.518207 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.610279 \n",
      "\n",
      "Epoch 15: Train_accuracy: 73.84%, Train_loss: 0.518207, Test_accuracy: 68.25%, Test_loss: 0.610279\n",
      "\n",
      "\n",
      "loss: 0.538842  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.506499 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.608512 \n",
      "\n",
      "Epoch 16: Train_accuracy: 74.58%, Train_loss: 0.506499, Test_accuracy: 68.31%, Test_loss: 0.608512\n",
      "\n",
      "\n",
      "loss: 0.506826  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.489243 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.618185 \n",
      "\n",
      "Epoch 17: Train_accuracy: 75.96%, Train_loss: 0.489243, Test_accuracy: 67.24%, Test_loss: 0.618185\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 3\n",
      "loss: 0.693195  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.5%, Avg loss: 0.688763 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.682259 \n",
      "\n",
      "Epoch 1: Train_accuracy: 53.47%, Train_loss: 0.688763, Test_accuracy: 56.40%, Test_loss: 0.682259\n",
      "\n",
      "\n",
      "loss: 0.674103  [  256/ 6751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.676201 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.679664 \n",
      "\n",
      "Epoch 2: Train_accuracy: 58.12%, Train_loss: 0.676201, Test_accuracy: 56.58%, Test_loss: 0.679664\n",
      "\n",
      "\n",
      "loss: 0.652921  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.661773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.676860 \n",
      "\n",
      "Epoch 3: Train_accuracy: 60.20%, Train_loss: 0.661773, Test_accuracy: 58.89%, Test_loss: 0.676860\n",
      "\n",
      "\n",
      "loss: 0.637718  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.650017 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.669101 \n",
      "\n",
      "Epoch 4: Train_accuracy: 62.02%, Train_loss: 0.650017, Test_accuracy: 58.35%, Test_loss: 0.669101\n",
      "\n",
      "\n",
      "loss: 0.639866  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.638087 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.661185 \n",
      "\n",
      "Epoch 5: Train_accuracy: 63.44%, Train_loss: 0.638087, Test_accuracy: 59.18%, Test_loss: 0.661185\n",
      "\n",
      "\n",
      "loss: 0.620934  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.618770 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.672633 \n",
      "\n",
      "Epoch 6: Train_accuracy: 65.61%, Train_loss: 0.618770, Test_accuracy: 59.89%, Test_loss: 0.672633\n",
      "\n",
      "\n",
      "loss: 0.587852  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.610193 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.662494 \n",
      "\n",
      "Epoch 7: Train_accuracy: 65.93%, Train_loss: 0.610193, Test_accuracy: 60.43%, Test_loss: 0.662494\n",
      "\n",
      "\n",
      "loss: 0.588456  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.601021 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.663846 \n",
      "\n",
      "Epoch 8: Train_accuracy: 67.40%, Train_loss: 0.601021, Test_accuracy: 62.20%, Test_loss: 0.663846\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 4\n",
      "loss: 0.695441  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 51.5%, Avg loss: 0.690614 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.686538 \n",
      "\n",
      "Epoch 1: Train_accuracy: 51.50%, Train_loss: 0.690614, Test_accuracy: 54.09%, Test_loss: 0.686538\n",
      "\n",
      "\n",
      "loss: 0.683648  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.679410 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.678166 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.40%, Train_loss: 0.679410, Test_accuracy: 55.86%, Test_loss: 0.678166\n",
      "\n",
      "\n",
      "loss: 0.672737  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.667717 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.667336 \n",
      "\n",
      "Epoch 3: Train_accuracy: 59.46%, Train_loss: 0.667717, Test_accuracy: 59.00%, Test_loss: 0.667336\n",
      "\n",
      "\n",
      "loss: 0.644952  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.655846 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.664011 \n",
      "\n",
      "Epoch 4: Train_accuracy: 61.50%, Train_loss: 0.655846, Test_accuracy: 59.42%, Test_loss: 0.664011\n",
      "\n",
      "\n",
      "loss: 0.649829  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.640625 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.659669 \n",
      "\n",
      "Epoch 5: Train_accuracy: 63.87%, Train_loss: 0.640625, Test_accuracy: 60.90%, Test_loss: 0.659669\n",
      "\n",
      "\n",
      "loss: 0.612857  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.632557 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.652422 \n",
      "\n",
      "Epoch 6: Train_accuracy: 63.99%, Train_loss: 0.632557, Test_accuracy: 61.67%, Test_loss: 0.652422\n",
      "\n",
      "\n",
      "loss: 0.623413  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.612818 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.652168 \n",
      "\n",
      "Epoch 7: Train_accuracy: 66.32%, Train_loss: 0.612818, Test_accuracy: 62.91%, Test_loss: 0.652168\n",
      "\n",
      "\n",
      "loss: 0.634258  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.606624 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.655619 \n",
      "\n",
      "Epoch 8: Train_accuracy: 67.22%, Train_loss: 0.606624, Test_accuracy: 62.50%, Test_loss: 0.655619\n",
      "\n",
      "\n",
      "loss: 0.546068  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.594965 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.643455 \n",
      "\n",
      "Epoch 9: Train_accuracy: 67.87%, Train_loss: 0.594965, Test_accuracy: 64.04%, Test_loss: 0.643455\n",
      "\n",
      "\n",
      "loss: 0.551698  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.571192 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.654190 \n",
      "\n",
      "Epoch 10: Train_accuracy: 70.51%, Train_loss: 0.571192, Test_accuracy: 63.45%, Test_loss: 0.654190\n",
      "\n",
      "\n",
      "loss: 0.558143  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.568227 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.638833 \n",
      "\n",
      "Epoch 11: Train_accuracy: 70.30%, Train_loss: 0.568227, Test_accuracy: 65.28%, Test_loss: 0.638833\n",
      "\n",
      "\n",
      "loss: 0.600158  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.554601 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.644597 \n",
      "\n",
      "Epoch 12: Train_accuracy: 72.15%, Train_loss: 0.554601, Test_accuracy: 65.28%, Test_loss: 0.644597\n",
      "\n",
      "\n",
      "loss: 0.581170  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.538821 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.654927 \n",
      "\n",
      "Epoch 13: Train_accuracy: 72.83%, Train_loss: 0.538821, Test_accuracy: 64.16%, Test_loss: 0.654927\n",
      "\n",
      "\n",
      "loss: 0.522300  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.535950 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.626255 \n",
      "\n",
      "Epoch 14: Train_accuracy: 71.86%, Train_loss: 0.535950, Test_accuracy: 67.12%, Test_loss: 0.626255\n",
      "\n",
      "\n",
      "loss: 0.564223  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.512301 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.646433 \n",
      "\n",
      "Epoch 15: Train_accuracy: 74.24%, Train_loss: 0.512301, Test_accuracy: 64.93%, Test_loss: 0.646433\n",
      "\n",
      "\n",
      "loss: 0.523408  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.512015 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.647763 \n",
      "\n",
      "Epoch 16: Train_accuracy: 74.86%, Train_loss: 0.512015, Test_accuracy: 65.23%, Test_loss: 0.647763\n",
      "\n",
      "\n",
      "loss: 0.530485  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.498643 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.652719 \n",
      "\n",
      "Epoch 17: Train_accuracy: 75.17%, Train_loss: 0.498643, Test_accuracy: 65.82%, Test_loss: 0.652719\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 5\n",
      "loss: 0.691644  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.688282 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.679806 \n",
      "\n",
      "Epoch 1: Train_accuracy: 54.13%, Train_loss: 0.688282, Test_accuracy: 57.50%, Test_loss: 0.679806\n",
      "\n",
      "\n",
      "loss: 0.686562  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.674544 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.672136 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.66%, Train_loss: 0.674544, Test_accuracy: 58.80%, Test_loss: 0.672136\n",
      "\n",
      "\n",
      "loss: 0.640902  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.662270 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.662048 \n",
      "\n",
      "Epoch 3: Train_accuracy: 60.77%, Train_loss: 0.662270, Test_accuracy: 60.94%, Test_loss: 0.662048\n",
      "\n",
      "\n",
      "loss: 0.630515  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.650193 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.663035 \n",
      "\n",
      "Epoch 4: Train_accuracy: 61.69%, Train_loss: 0.650193, Test_accuracy: 60.46%, Test_loss: 0.663035\n",
      "\n",
      "\n",
      "loss: 0.656091  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.636946 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.671490 \n",
      "\n",
      "Epoch 5: Train_accuracy: 63.63%, Train_loss: 0.636946, Test_accuracy: 59.28%, Test_loss: 0.671490\n",
      "\n",
      "\n",
      "loss: 0.622224  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.632330 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.647011 \n",
      "\n",
      "Epoch 6: Train_accuracy: 64.13%, Train_loss: 0.632330, Test_accuracy: 61.71%, Test_loss: 0.647011\n",
      "\n",
      "\n",
      "loss: 0.631942  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.616084 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.650184 \n",
      "\n",
      "Epoch 7: Train_accuracy: 65.67%, Train_loss: 0.616084, Test_accuracy: 61.94%, Test_loss: 0.650184\n",
      "\n",
      "\n",
      "loss: 0.613557  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.603563 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.648837 \n",
      "\n",
      "Epoch 8: Train_accuracy: 67.28%, Train_loss: 0.603563, Test_accuracy: 63.78%, Test_loss: 0.648837\n",
      "\n",
      "\n",
      "loss: 0.574728  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.588362 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.640992 \n",
      "\n",
      "Epoch 9: Train_accuracy: 68.68%, Train_loss: 0.588362, Test_accuracy: 63.96%, Test_loss: 0.640992\n",
      "\n",
      "\n",
      "loss: 0.583158  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579125 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.639906 \n",
      "\n",
      "Epoch 10: Train_accuracy: 69.45%, Train_loss: 0.579125, Test_accuracy: 63.66%, Test_loss: 0.639906\n",
      "\n",
      "\n",
      "loss: 0.571575  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.560602 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.618107 \n",
      "\n",
      "Epoch 11: Train_accuracy: 71.00%, Train_loss: 0.560602, Test_accuracy: 65.62%, Test_loss: 0.618107\n",
      "\n",
      "\n",
      "loss: 0.528418  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.553830 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.625694 \n",
      "\n",
      "Epoch 12: Train_accuracy: 71.02%, Train_loss: 0.553830, Test_accuracy: 66.09%, Test_loss: 0.625694\n",
      "\n",
      "\n",
      "loss: 0.540414  [  256/ 6752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.542232 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.642966 \n",
      "\n",
      "Epoch 13: Train_accuracy: 72.45%, Train_loss: 0.542232, Test_accuracy: 65.56%, Test_loss: 0.642966\n",
      "\n",
      "\n",
      "loss: 0.512577  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.536365 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.615496 \n",
      "\n",
      "Epoch 14: Train_accuracy: 72.84%, Train_loss: 0.536365, Test_accuracy: 66.63%, Test_loss: 0.615496\n",
      "\n",
      "\n",
      "loss: 0.511763  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.508934 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.642080 \n",
      "\n",
      "Epoch 15: Train_accuracy: 74.69%, Train_loss: 0.508934, Test_accuracy: 65.38%, Test_loss: 0.642080\n",
      "\n",
      "\n",
      "loss: 0.522009  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.502155 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.637717 \n",
      "\n",
      "Epoch 16: Train_accuracy: 75.58%, Train_loss: 0.502155, Test_accuracy: 66.39%, Test_loss: 0.637717\n",
      "\n",
      "\n",
      "loss: 0.488840  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.488714 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.630248 \n",
      "\n",
      "Epoch 17: Train_accuracy: 76.41%, Train_loss: 0.488714, Test_accuracy: 66.57%, Test_loss: 0.630248\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "512\n",
      "----------------------------------\n",
      "\n",
      "\n",
      "Experiment 1\n",
      "loss: 0.695102  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 52.4%, Avg loss: 0.690256 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.687069 \n",
      "\n",
      "Epoch 1: Train_accuracy: 52.42%, Train_loss: 0.690256, Test_accuracy: 55.39%, Test_loss: 0.687069\n",
      "\n",
      "\n",
      "loss: 0.686684  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.678613 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.684602 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.01%, Train_loss: 0.678613, Test_accuracy: 57.05%, Test_loss: 0.684602\n",
      "\n",
      "\n",
      "loss: 0.688636  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.674318 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.681008 \n",
      "\n",
      "Epoch 3: Train_accuracy: 58.20%, Train_loss: 0.674318, Test_accuracy: 58.41%, Test_loss: 0.681008\n",
      "\n",
      "\n",
      "loss: 0.670031  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.665387 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.672877 \n",
      "\n",
      "Epoch 4: Train_accuracy: 59.52%, Train_loss: 0.665387, Test_accuracy: 59.12%, Test_loss: 0.672877\n",
      "\n",
      "\n",
      "loss: 0.658856  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.654027 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.668083 \n",
      "\n",
      "Epoch 5: Train_accuracy: 60.89%, Train_loss: 0.654027, Test_accuracy: 59.00%, Test_loss: 0.668083\n",
      "\n",
      "\n",
      "loss: 0.654774  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.650886 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.663583 \n",
      "\n",
      "Epoch 6: Train_accuracy: 62.73%, Train_loss: 0.650886, Test_accuracy: 60.31%, Test_loss: 0.663583\n",
      "\n",
      "\n",
      "loss: 0.646083  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.639614 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.659057 \n",
      "\n",
      "Epoch 7: Train_accuracy: 63.25%, Train_loss: 0.639614, Test_accuracy: 61.91%, Test_loss: 0.659057\n",
      "\n",
      "\n",
      "loss: 0.632259  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.625082 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.651059 \n",
      "\n",
      "Epoch 8: Train_accuracy: 64.83%, Train_loss: 0.625082, Test_accuracy: 61.61%, Test_loss: 0.651059\n",
      "\n",
      "\n",
      "loss: 0.619935  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.616660 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.653154 \n",
      "\n",
      "Epoch 9: Train_accuracy: 64.76%, Train_loss: 0.616660, Test_accuracy: 62.50%, Test_loss: 0.653154\n",
      "\n",
      "\n",
      "loss: 0.643725  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.615199 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.645867 \n",
      "\n",
      "Epoch 10: Train_accuracy: 65.49%, Train_loss: 0.615199, Test_accuracy: 62.86%, Test_loss: 0.645867\n",
      "\n",
      "\n",
      "loss: 0.584188  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.601825 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.644586 \n",
      "\n",
      "Epoch 11: Train_accuracy: 67.15%, Train_loss: 0.601825, Test_accuracy: 62.26%, Test_loss: 0.644586\n",
      "\n",
      "\n",
      "loss: 0.604039  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.586095 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.666339 \n",
      "\n",
      "Epoch 12: Train_accuracy: 68.42%, Train_loss: 0.586095, Test_accuracy: 61.14%, Test_loss: 0.666339\n",
      "\n",
      "\n",
      "loss: 0.603645  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.594273 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.645403 \n",
      "\n",
      "Epoch 13: Train_accuracy: 68.08%, Train_loss: 0.594273, Test_accuracy: 63.51%, Test_loss: 0.645403\n",
      "\n",
      "\n",
      "loss: 0.543039  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.569213 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.641265 \n",
      "\n",
      "Epoch 14: Train_accuracy: 69.62%, Train_loss: 0.569213, Test_accuracy: 63.27%, Test_loss: 0.641265\n",
      "\n",
      "\n",
      "loss: 0.578061  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.576938 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.644609 \n",
      "\n",
      "Epoch 15: Train_accuracy: 70.02%, Train_loss: 0.576938, Test_accuracy: 64.10%, Test_loss: 0.644609\n",
      "\n",
      "\n",
      "loss: 0.552936  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.562175 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.639811 \n",
      "\n",
      "Epoch 16: Train_accuracy: 70.92%, Train_loss: 0.562175, Test_accuracy: 65.11%, Test_loss: 0.639811\n",
      "\n",
      "\n",
      "loss: 0.559388  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.552034 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.649950 \n",
      "\n",
      "Epoch 17: Train_accuracy: 71.28%, Train_loss: 0.552034, Test_accuracy: 64.10%, Test_loss: 0.649950\n",
      "\n",
      "\n",
      "loss: 0.527200  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.546072 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.626804 \n",
      "\n",
      "Epoch 18: Train_accuracy: 71.72%, Train_loss: 0.546072, Test_accuracy: 65.64%, Test_loss: 0.626804\n",
      "\n",
      "\n",
      "loss: 0.513546  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.534962 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.632390 \n",
      "\n",
      "Epoch 19: Train_accuracy: 73.03%, Train_loss: 0.534962, Test_accuracy: 66.29%, Test_loss: 0.632390\n",
      "\n",
      "\n",
      "loss: 0.515266  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.529604 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.640510 \n",
      "\n",
      "Epoch 20: Train_accuracy: 72.98%, Train_loss: 0.529604, Test_accuracy: 66.41%, Test_loss: 0.640510\n",
      "\n",
      "\n",
      "loss: 0.513494  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.514478 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.637885 \n",
      "\n",
      "Epoch 21: Train_accuracy: 73.99%, Train_loss: 0.514478, Test_accuracy: 65.40%, Test_loss: 0.637885\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 2\n",
      "loss: 0.692889  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 52.5%, Avg loss: 0.690433 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.687656 \n",
      "\n",
      "Epoch 1: Train_accuracy: 52.50%, Train_loss: 0.690433, Test_accuracy: 53.73%, Test_loss: 0.687656\n",
      "\n",
      "\n",
      "loss: 0.686803  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.681746 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.682703 \n",
      "\n",
      "Epoch 2: Train_accuracy: 56.17%, Train_loss: 0.681746, Test_accuracy: 55.45%, Test_loss: 0.682703\n",
      "\n",
      "\n",
      "loss: 0.674214  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.675014 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.677479 \n",
      "\n",
      "Epoch 3: Train_accuracy: 58.04%, Train_loss: 0.675014, Test_accuracy: 56.22%, Test_loss: 0.677479\n",
      "\n",
      "\n",
      "loss: 0.663223  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.663682 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.674252 \n",
      "\n",
      "Epoch 4: Train_accuracy: 59.50%, Train_loss: 0.663682, Test_accuracy: 57.17%, Test_loss: 0.674252\n",
      "\n",
      "\n",
      "loss: 0.664475  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.661456 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.675287 \n",
      "\n",
      "Epoch 5: Train_accuracy: 60.23%, Train_loss: 0.661456, Test_accuracy: 56.81%, Test_loss: 0.675287\n",
      "\n",
      "\n",
      "loss: 0.650117  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.652053 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.667382 \n",
      "\n",
      "Epoch 6: Train_accuracy: 62.41%, Train_loss: 0.652053, Test_accuracy: 59.89%, Test_loss: 0.667382\n",
      "\n",
      "\n",
      "loss: 0.660611  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.646709 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.653293 \n",
      "\n",
      "Epoch 7: Train_accuracy: 61.80%, Train_loss: 0.646709, Test_accuracy: 61.79%, Test_loss: 0.653293\n",
      "\n",
      "\n",
      "loss: 0.640838  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.642872 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.639129 \n",
      "\n",
      "Epoch 8: Train_accuracy: 63.13%, Train_loss: 0.642872, Test_accuracy: 62.62%, Test_loss: 0.639129\n",
      "\n",
      "\n",
      "loss: 0.617043  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.625911 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.639678 \n",
      "\n",
      "Epoch 9: Train_accuracy: 64.64%, Train_loss: 0.625911, Test_accuracy: 62.97%, Test_loss: 0.639678\n",
      "\n",
      "\n",
      "loss: 0.598898  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.620280 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.647453 \n",
      "\n",
      "Epoch 10: Train_accuracy: 65.71%, Train_loss: 0.620280, Test_accuracy: 62.38%, Test_loss: 0.647453\n",
      "\n",
      "\n",
      "loss: 0.615193  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.610505 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.637826 \n",
      "\n",
      "Epoch 11: Train_accuracy: 67.06%, Train_loss: 0.610505, Test_accuracy: 63.21%, Test_loss: 0.637826\n",
      "\n",
      "\n",
      "loss: 0.581092  [  512/ 6751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.598220 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.638534 \n",
      "\n",
      "Epoch 12: Train_accuracy: 68.18%, Train_loss: 0.598220, Test_accuracy: 64.10%, Test_loss: 0.638534\n",
      "\n",
      "\n",
      "loss: 0.585601  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.581865 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.624791 \n",
      "\n",
      "Epoch 13: Train_accuracy: 68.98%, Train_loss: 0.581865, Test_accuracy: 63.57%, Test_loss: 0.624791\n",
      "\n",
      "\n",
      "loss: 0.570358  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.585123 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.626195 \n",
      "\n",
      "Epoch 14: Train_accuracy: 69.26%, Train_loss: 0.585123, Test_accuracy: 64.57%, Test_loss: 0.626195\n",
      "\n",
      "\n",
      "loss: 0.565979  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.568808 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.618208 \n",
      "\n",
      "Epoch 15: Train_accuracy: 70.12%, Train_loss: 0.568808, Test_accuracy: 67.42%, Test_loss: 0.618208\n",
      "\n",
      "\n",
      "loss: 0.540509  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.557891 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.626660 \n",
      "\n",
      "Epoch 16: Train_accuracy: 70.32%, Train_loss: 0.557891, Test_accuracy: 66.82%, Test_loss: 0.626660\n",
      "\n",
      "\n",
      "loss: 0.573236  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.548235 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.655943 \n",
      "\n",
      "Epoch 17: Train_accuracy: 72.66%, Train_loss: 0.548235, Test_accuracy: 64.63%, Test_loss: 0.655943\n",
      "\n",
      "\n",
      "loss: 0.552656  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.539059 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.635163 \n",
      "\n",
      "Epoch 18: Train_accuracy: 72.80%, Train_loss: 0.539059, Test_accuracy: 66.17%, Test_loss: 0.635163\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 3\n",
      "loss: 0.689639  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 52.7%, Avg loss: 0.690816 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.685772 \n",
      "\n",
      "Epoch 1: Train_accuracy: 52.67%, Train_loss: 0.690816, Test_accuracy: 56.34%, Test_loss: 0.685772\n",
      "\n",
      "\n",
      "loss: 0.680813  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.681783 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.688626 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.13%, Train_loss: 0.681783, Test_accuracy: 55.69%, Test_loss: 0.688626\n",
      "\n",
      "\n",
      "loss: 0.668496  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.672117 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.682731 \n",
      "\n",
      "Epoch 3: Train_accuracy: 58.78%, Train_loss: 0.672117, Test_accuracy: 56.87%, Test_loss: 0.682731\n",
      "\n",
      "\n",
      "loss: 0.656073  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.664905 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.682333 \n",
      "\n",
      "Epoch 4: Train_accuracy: 59.81%, Train_loss: 0.664905, Test_accuracy: 58.41%, Test_loss: 0.682333\n",
      "\n",
      "\n",
      "loss: 0.650780  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.660212 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.667987 \n",
      "\n",
      "Epoch 5: Train_accuracy: 59.93%, Train_loss: 0.660212, Test_accuracy: 58.23%, Test_loss: 0.667987\n",
      "\n",
      "\n",
      "loss: 0.657722  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.650816 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.672851 \n",
      "\n",
      "Epoch 6: Train_accuracy: 61.99%, Train_loss: 0.650816, Test_accuracy: 58.77%, Test_loss: 0.672851\n",
      "\n",
      "\n",
      "loss: 0.642191  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.640793 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.677641 \n",
      "\n",
      "Epoch 7: Train_accuracy: 62.84%, Train_loss: 0.640793, Test_accuracy: 58.71%, Test_loss: 0.677641\n",
      "\n",
      "\n",
      "loss: 0.636963  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.628858 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.672418 \n",
      "\n",
      "Epoch 8: Train_accuracy: 64.20%, Train_loss: 0.628858, Test_accuracy: 59.48%, Test_loss: 0.672418\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 4\n",
      "loss: 0.691161  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.9%, Avg loss: 0.688413 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.687128 \n",
      "\n",
      "Epoch 1: Train_accuracy: 53.90%, Train_loss: 0.688413, Test_accuracy: 54.27%, Test_loss: 0.687128\n",
      "\n",
      "\n",
      "loss: 0.683529  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.677820 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.678034 \n",
      "\n",
      "Epoch 2: Train_accuracy: 57.16%, Train_loss: 0.677820, Test_accuracy: 57.41%, Test_loss: 0.678034\n",
      "\n",
      "\n",
      "loss: 0.679737  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.670873 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.677508 \n",
      "\n",
      "Epoch 3: Train_accuracy: 59.28%, Train_loss: 0.670873, Test_accuracy: 57.52%, Test_loss: 0.677508\n",
      "\n",
      "\n",
      "loss: 0.669614  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.662910 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.677891 \n",
      "\n",
      "Epoch 4: Train_accuracy: 60.69%, Train_loss: 0.662910, Test_accuracy: 58.41%, Test_loss: 0.677891\n",
      "\n",
      "\n",
      "loss: 0.664191  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.654488 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.665742 \n",
      "\n",
      "Epoch 5: Train_accuracy: 61.03%, Train_loss: 0.654488, Test_accuracy: 59.77%, Test_loss: 0.665742\n",
      "\n",
      "\n",
      "loss: 0.643721  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.640470 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.666200 \n",
      "\n",
      "Epoch 6: Train_accuracy: 62.48%, Train_loss: 0.640470, Test_accuracy: 60.72%, Test_loss: 0.666200\n",
      "\n",
      "\n",
      "loss: 0.624856  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.635173 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.667764 \n",
      "\n",
      "Epoch 7: Train_accuracy: 63.75%, Train_loss: 0.635173, Test_accuracy: 61.08%, Test_loss: 0.667764\n",
      "\n",
      "\n",
      "loss: 0.641026  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.630060 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.663651 \n",
      "\n",
      "Epoch 8: Train_accuracy: 64.09%, Train_loss: 0.630060, Test_accuracy: 60.43%, Test_loss: 0.663651\n",
      "\n",
      "\n",
      "loss: 0.618208  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.621647 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.662034 \n",
      "\n",
      "Epoch 9: Train_accuracy: 65.49%, Train_loss: 0.621647, Test_accuracy: 60.60%, Test_loss: 0.662034\n",
      "\n",
      "\n",
      "loss: 0.581815  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.607268 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.652903 \n",
      "\n",
      "Epoch 10: Train_accuracy: 67.15%, Train_loss: 0.607268, Test_accuracy: 62.26%, Test_loss: 0.652903\n",
      "\n",
      "\n",
      "loss: 0.608605  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.597930 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.652009 \n",
      "\n",
      "Epoch 11: Train_accuracy: 67.53%, Train_loss: 0.597930, Test_accuracy: 63.21%, Test_loss: 0.652009\n",
      "\n",
      "\n",
      "loss: 0.571765  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.584882 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.651891 \n",
      "\n",
      "Epoch 12: Train_accuracy: 68.20%, Train_loss: 0.584882, Test_accuracy: 62.74%, Test_loss: 0.651891\n",
      "\n",
      "\n",
      "loss: 0.537269  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.576903 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.651603 \n",
      "\n",
      "Epoch 13: Train_accuracy: 69.37%, Train_loss: 0.576903, Test_accuracy: 63.09%, Test_loss: 0.651603\n",
      "\n",
      "\n",
      "loss: 0.575884  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.574155 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.664897 \n",
      "\n",
      "Epoch 14: Train_accuracy: 69.96%, Train_loss: 0.574155, Test_accuracy: 63.39%, Test_loss: 0.664897\n",
      "\n",
      "\n",
      "loss: 0.521257  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.561055 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.662696 \n",
      "\n",
      "Epoch 15: Train_accuracy: 70.82%, Train_loss: 0.561055, Test_accuracy: 62.80%, Test_loss: 0.662696\n",
      "\n",
      "\n",
      "loss: 0.560439  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.551898 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.639787 \n",
      "\n",
      "Epoch 16: Train_accuracy: 71.74%, Train_loss: 0.551898, Test_accuracy: 65.94%, Test_loss: 0.639787\n",
      "\n",
      "\n",
      "loss: 0.522992  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.545024 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.619834 \n",
      "\n",
      "Epoch 17: Train_accuracy: 71.92%, Train_loss: 0.545024, Test_accuracy: 64.75%, Test_loss: 0.619834\n",
      "\n",
      "\n",
      "loss: 0.523486  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.530761 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.650470 \n",
      "\n",
      "Epoch 18: Train_accuracy: 73.00%, Train_loss: 0.530761, Test_accuracy: 65.94%, Test_loss: 0.650470\n",
      "\n",
      "\n",
      "loss: 0.494867  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.527284 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.641223 \n",
      "\n",
      "Epoch 19: Train_accuracy: 73.54%, Train_loss: 0.527284, Test_accuracy: 65.70%, Test_loss: 0.641223\n",
      "\n",
      "\n",
      "loss: 0.490083  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.520895 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.626483 \n",
      "\n",
      "Epoch 20: Train_accuracy: 74.30%, Train_loss: 0.520895, Test_accuracy: 66.35%, Test_loss: 0.626483\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 5\n",
      "loss: 0.690127  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.691640 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.686089 \n",
      "\n",
      "Epoch 1: Train_accuracy: 51.91%, Train_loss: 0.691640, Test_accuracy: 55.54%, Test_loss: 0.686089\n",
      "\n",
      "\n",
      "loss: 0.685061  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.683163 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.683396 \n",
      "\n",
      "Epoch 2: Train_accuracy: 56.46%, Train_loss: 0.683163, Test_accuracy: 56.97%, Test_loss: 0.683396\n",
      "\n",
      "\n",
      "loss: 0.677833  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.672503 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.673960 \n",
      "\n",
      "Epoch 3: Train_accuracy: 58.06%, Train_loss: 0.672503, Test_accuracy: 57.14%, Test_loss: 0.673960\n",
      "\n",
      "\n",
      "loss: 0.668766  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.661113 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.663519 \n",
      "\n",
      "Epoch 4: Train_accuracy: 59.76%, Train_loss: 0.661113, Test_accuracy: 58.98%, Test_loss: 0.663519\n",
      "\n",
      "\n",
      "loss: 0.670450  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.656705 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.665909 \n",
      "\n",
      "Epoch 5: Train_accuracy: 61.89%, Train_loss: 0.656705, Test_accuracy: 60.52%, Test_loss: 0.665909\n",
      "\n",
      "\n",
      "loss: 0.614527  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.647021 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.662530 \n",
      "\n",
      "Epoch 6: Train_accuracy: 63.26%, Train_loss: 0.647021, Test_accuracy: 59.99%, Test_loss: 0.662530\n",
      "\n",
      "\n",
      "loss: 0.640426  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.636020 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.670325 \n",
      "\n",
      "Epoch 7: Train_accuracy: 63.97%, Train_loss: 0.636020, Test_accuracy: 60.52%, Test_loss: 0.670325\n",
      "\n",
      "\n",
      "loss: 0.632050  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.630092 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.667269 \n",
      "\n",
      "Epoch 8: Train_accuracy: 63.94%, Train_loss: 0.630092, Test_accuracy: 60.82%, Test_loss: 0.667269\n",
      "\n",
      "\n",
      "loss: 0.637818  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.623112 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.655364 \n",
      "\n",
      "Epoch 9: Train_accuracy: 64.74%, Train_loss: 0.623112, Test_accuracy: 61.11%, Test_loss: 0.655364\n",
      "\n",
      "\n",
      "loss: 0.623544  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.608353 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.663744 \n",
      "\n",
      "Epoch 10: Train_accuracy: 66.53%, Train_loss: 0.608353, Test_accuracy: 59.63%, Test_loss: 0.663744\n",
      "\n",
      "\n",
      "loss: 0.621362  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.606769 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.651717 \n",
      "\n",
      "Epoch 11: Train_accuracy: 67.03%, Train_loss: 0.606769, Test_accuracy: 62.66%, Test_loss: 0.651717\n",
      "\n",
      "\n",
      "loss: 0.587003  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.597002 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.648293 \n",
      "\n",
      "Epoch 12: Train_accuracy: 67.70%, Train_loss: 0.597002, Test_accuracy: 62.89%, Test_loss: 0.648293\n",
      "\n",
      "\n",
      "loss: 0.576833  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.584102 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.648845 \n",
      "\n",
      "Epoch 13: Train_accuracy: 69.06%, Train_loss: 0.584102, Test_accuracy: 62.30%, Test_loss: 0.648845\n",
      "\n",
      "\n",
      "loss: 0.568762  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.576151 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.653192 \n",
      "\n",
      "Epoch 14: Train_accuracy: 69.42%, Train_loss: 0.576151, Test_accuracy: 61.59%, Test_loss: 0.653192\n",
      "\n",
      "\n",
      "loss: 0.565288  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.572282 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.656102 \n",
      "\n",
      "Epoch 15: Train_accuracy: 70.17%, Train_loss: 0.572282, Test_accuracy: 62.30%, Test_loss: 0.656102\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "1024\n",
      "----------------------------------\n",
      "\n",
      "\n",
      "Experiment 1\n",
      "loss: 0.695542  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 50.5%, Avg loss: 0.692463 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.691671 \n",
      "\n",
      "Epoch 1: Train_accuracy: 50.48%, Train_loss: 0.692463, Test_accuracy: 51.78%, Test_loss: 0.691671\n",
      "\n",
      "\n",
      "loss: 0.688778  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.686113 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.685301 \n",
      "\n",
      "Epoch 2: Train_accuracy: 55.46%, Train_loss: 0.686113, Test_accuracy: 56.40%, Test_loss: 0.685301\n",
      "\n",
      "\n",
      "loss: 0.681897  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.681020 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.680408 \n",
      "\n",
      "Epoch 3: Train_accuracy: 57.28%, Train_loss: 0.681020, Test_accuracy: 57.58%, Test_loss: 0.680408\n",
      "\n",
      "\n",
      "loss: 0.671521  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.673949 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.677931 \n",
      "\n",
      "Epoch 4: Train_accuracy: 57.90%, Train_loss: 0.673949, Test_accuracy: 58.77%, Test_loss: 0.677931\n",
      "\n",
      "\n",
      "loss: 0.667132  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.666479 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.668802 \n",
      "\n",
      "Epoch 5: Train_accuracy: 59.50%, Train_loss: 0.666479, Test_accuracy: 59.36%, Test_loss: 0.668802\n",
      "\n",
      "\n",
      "loss: 0.671957  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.660375 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.667620 \n",
      "\n",
      "Epoch 6: Train_accuracy: 60.61%, Train_loss: 0.660375, Test_accuracy: 59.30%, Test_loss: 0.667620\n",
      "\n",
      "\n",
      "loss: 0.657853  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.652454 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.667470 \n",
      "\n",
      "Epoch 7: Train_accuracy: 61.50%, Train_loss: 0.652454, Test_accuracy: 58.83%, Test_loss: 0.667470\n",
      "\n",
      "\n",
      "loss: 0.657925  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.646450 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.661947 \n",
      "\n",
      "Epoch 8: Train_accuracy: 62.67%, Train_loss: 0.646450, Test_accuracy: 60.43%, Test_loss: 0.661947\n",
      "\n",
      "\n",
      "loss: 0.638609  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.634091 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.661271 \n",
      "\n",
      "Epoch 9: Train_accuracy: 63.80%, Train_loss: 0.634091, Test_accuracy: 61.61%, Test_loss: 0.661271\n",
      "\n",
      "\n",
      "loss: 0.634833  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.630916 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.643887 \n",
      "\n",
      "Epoch 10: Train_accuracy: 64.11%, Train_loss: 0.630916, Test_accuracy: 61.49%, Test_loss: 0.643887\n",
      "\n",
      "\n",
      "loss: 0.615988  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.620667 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.654189 \n",
      "\n",
      "Epoch 11: Train_accuracy: 65.55%, Train_loss: 0.620667, Test_accuracy: 61.02%, Test_loss: 0.654189\n",
      "\n",
      "\n",
      "loss: 0.605923  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.619823 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.643258 \n",
      "\n",
      "Epoch 12: Train_accuracy: 65.53%, Train_loss: 0.619823, Test_accuracy: 63.57%, Test_loss: 0.643258\n",
      "\n",
      "\n",
      "loss: 0.628297  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.608112 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.643482 \n",
      "\n",
      "Epoch 13: Train_accuracy: 66.89%, Train_loss: 0.608112, Test_accuracy: 64.10%, Test_loss: 0.643482\n",
      "\n",
      "\n",
      "loss: 0.586716  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.601011 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.646148 \n",
      "\n",
      "Epoch 14: Train_accuracy: 67.20%, Train_loss: 0.601011, Test_accuracy: 61.37%, Test_loss: 0.646148\n",
      "\n",
      "\n",
      "loss: 0.586601  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.594744 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.646935 \n",
      "\n",
      "Epoch 15: Train_accuracy: 68.08%, Train_loss: 0.594744, Test_accuracy: 63.51%, Test_loss: 0.646935\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 2\n",
      "loss: 0.694330  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 52.0%, Avg loss: 0.691275 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.689604 \n",
      "\n",
      "Epoch 1: Train_accuracy: 52.02%, Train_loss: 0.691275, Test_accuracy: 53.67%, Test_loss: 0.689604\n",
      "\n",
      "\n",
      "loss: 0.690395  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.8%, Avg loss: 0.685800 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.684973 \n",
      "\n",
      "Epoch 2: Train_accuracy: 54.84%, Train_loss: 0.685800, Test_accuracy: 55.86%, Test_loss: 0.684973\n",
      "\n",
      "\n",
      "loss: 0.676312  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.681001 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.684204 \n",
      "\n",
      "Epoch 3: Train_accuracy: 56.85%, Train_loss: 0.681001, Test_accuracy: 56.16%, Test_loss: 0.684204\n",
      "\n",
      "\n",
      "loss: 0.678485  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.672705 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.674089 \n",
      "\n",
      "Epoch 4: Train_accuracy: 58.60%, Train_loss: 0.672705, Test_accuracy: 56.87%, Test_loss: 0.674089\n",
      "\n",
      "\n",
      "loss: 0.678710  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.669656 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.675040 \n",
      "\n",
      "Epoch 5: Train_accuracy: 58.92%, Train_loss: 0.669656, Test_accuracy: 58.83%, Test_loss: 0.675040\n",
      "\n",
      "\n",
      "loss: 0.666436  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.663699 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.666358 \n",
      "\n",
      "Epoch 6: Train_accuracy: 59.87%, Train_loss: 0.663699, Test_accuracy: 59.12%, Test_loss: 0.666358\n",
      "\n",
      "\n",
      "loss: 0.656317  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.658224 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.668260 \n",
      "\n",
      "Epoch 7: Train_accuracy: 60.81%, Train_loss: 0.658224, Test_accuracy: 58.77%, Test_loss: 0.668260\n",
      "\n",
      "\n",
      "loss: 0.644667  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.653325 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.659360 \n",
      "\n",
      "Epoch 8: Train_accuracy: 61.40%, Train_loss: 0.653325, Test_accuracy: 59.66%, Test_loss: 0.659360\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.636501  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.643065 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.654279 \n",
      "\n",
      "Epoch 9: Train_accuracy: 62.78%, Train_loss: 0.643065, Test_accuracy: 61.37%, Test_loss: 0.654279\n",
      "\n",
      "\n",
      "loss: 0.628926  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.634546 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.648604 \n",
      "\n",
      "Epoch 10: Train_accuracy: 64.18%, Train_loss: 0.634546, Test_accuracy: 61.67%, Test_loss: 0.648604\n",
      "\n",
      "\n",
      "loss: 0.624181  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.627137 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.652780 \n",
      "\n",
      "Epoch 11: Train_accuracy: 65.50%, Train_loss: 0.627137, Test_accuracy: 61.61%, Test_loss: 0.652780\n",
      "\n",
      "\n",
      "loss: 0.629669  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.620852 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.646656 \n",
      "\n",
      "Epoch 12: Train_accuracy: 65.80%, Train_loss: 0.620852, Test_accuracy: 62.14%, Test_loss: 0.646656\n",
      "\n",
      "\n",
      "loss: 0.626491  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.613528 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.638479 \n",
      "\n",
      "Epoch 13: Train_accuracy: 66.26%, Train_loss: 0.613528, Test_accuracy: 62.80%, Test_loss: 0.638479\n",
      "\n",
      "\n",
      "loss: 0.612749  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.611038 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.645840 \n",
      "\n",
      "Epoch 14: Train_accuracy: 66.79%, Train_loss: 0.611038, Test_accuracy: 63.03%, Test_loss: 0.645840\n",
      "\n",
      "\n",
      "loss: 0.599678  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.601114 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.631876 \n",
      "\n",
      "Epoch 15: Train_accuracy: 67.77%, Train_loss: 0.601114, Test_accuracy: 63.09%, Test_loss: 0.631876\n",
      "\n",
      "\n",
      "loss: 0.599434  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.590988 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.628905 \n",
      "\n",
      "Epoch 16: Train_accuracy: 68.45%, Train_loss: 0.590988, Test_accuracy: 64.51%, Test_loss: 0.628905\n",
      "\n",
      "\n",
      "loss: 0.575217  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.581825 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.625915 \n",
      "\n",
      "Epoch 17: Train_accuracy: 69.25%, Train_loss: 0.581825, Test_accuracy: 64.34%, Test_loss: 0.625915\n",
      "\n",
      "\n",
      "loss: 0.561222  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.575586 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.631180 \n",
      "\n",
      "Epoch 18: Train_accuracy: 69.10%, Train_loss: 0.575586, Test_accuracy: 65.05%, Test_loss: 0.631180\n",
      "\n",
      "\n",
      "loss: 0.563761  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.570985 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.610263 \n",
      "\n",
      "Epoch 19: Train_accuracy: 70.05%, Train_loss: 0.570985, Test_accuracy: 66.59%, Test_loss: 0.610263\n",
      "\n",
      "\n",
      "loss: 0.587430  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.559616 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.623285 \n",
      "\n",
      "Epoch 20: Train_accuracy: 71.32%, Train_loss: 0.559616, Test_accuracy: 66.47%, Test_loss: 0.623285\n",
      "\n",
      "\n",
      "loss: 0.541696  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.553877 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.622288 \n",
      "\n",
      "Epoch 21: Train_accuracy: 71.93%, Train_loss: 0.553877, Test_accuracy: 65.23%, Test_loss: 0.622288\n",
      "\n",
      "\n",
      "loss: 0.546167  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.547572 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.627867 \n",
      "\n",
      "Epoch 22: Train_accuracy: 72.34%, Train_loss: 0.547572, Test_accuracy: 65.11%, Test_loss: 0.627867\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 3\n",
      "loss: 0.694295  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 52.1%, Avg loss: 0.691347 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.688484 \n",
      "\n",
      "Epoch 1: Train_accuracy: 52.10%, Train_loss: 0.691347, Test_accuracy: 52.90%, Test_loss: 0.688484\n",
      "\n",
      "\n",
      "loss: 0.687089  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685258 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.683444 \n",
      "\n",
      "Epoch 2: Train_accuracy: 55.98%, Train_loss: 0.685258, Test_accuracy: 56.16%, Test_loss: 0.683444\n",
      "\n",
      "\n",
      "loss: 0.680695  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.679601 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.680401 \n",
      "\n",
      "Epoch 3: Train_accuracy: 57.04%, Train_loss: 0.679601, Test_accuracy: 56.93%, Test_loss: 0.680401\n",
      "\n",
      "\n",
      "loss: 0.681240  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.672175 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.678469 \n",
      "\n",
      "Epoch 4: Train_accuracy: 58.38%, Train_loss: 0.672175, Test_accuracy: 55.92%, Test_loss: 0.678469\n",
      "\n",
      "\n",
      "loss: 0.667728  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.666737 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.679123 \n",
      "\n",
      "Epoch 5: Train_accuracy: 59.47%, Train_loss: 0.666737, Test_accuracy: 57.23%, Test_loss: 0.679123\n",
      "\n",
      "\n",
      "loss: 0.664501  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.658371 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.676247 \n",
      "\n",
      "Epoch 6: Train_accuracy: 61.07%, Train_loss: 0.658371, Test_accuracy: 58.53%, Test_loss: 0.676247\n",
      "\n",
      "\n",
      "loss: 0.644312  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.650444 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.674690 \n",
      "\n",
      "Epoch 7: Train_accuracy: 61.81%, Train_loss: 0.650444, Test_accuracy: 59.24%, Test_loss: 0.674690\n",
      "\n",
      "\n",
      "loss: 0.629441  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.640653 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.672780 \n",
      "\n",
      "Epoch 8: Train_accuracy: 62.72%, Train_loss: 0.640653, Test_accuracy: 59.54%, Test_loss: 0.672780\n",
      "\n",
      "\n",
      "loss: 0.632368  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.635354 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.667611 \n",
      "\n",
      "Epoch 9: Train_accuracy: 63.66%, Train_loss: 0.635354, Test_accuracy: 59.06%, Test_loss: 0.667611\n",
      "\n",
      "\n",
      "loss: 0.626079  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.626337 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.668114 \n",
      "\n",
      "Epoch 10: Train_accuracy: 64.73%, Train_loss: 0.626337, Test_accuracy: 60.55%, Test_loss: 0.668114\n",
      "\n",
      "\n",
      "loss: 0.620273  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.620356 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.672285 \n",
      "\n",
      "Epoch 11: Train_accuracy: 65.62%, Train_loss: 0.620356, Test_accuracy: 60.19%, Test_loss: 0.672285\n",
      "\n",
      "\n",
      "loss: 0.611703  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.613582 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.656816 \n",
      "\n",
      "Epoch 12: Train_accuracy: 66.24%, Train_loss: 0.613582, Test_accuracy: 61.73%, Test_loss: 0.656816\n",
      "\n",
      "\n",
      "loss: 0.591990  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.599628 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.660244 \n",
      "\n",
      "Epoch 13: Train_accuracy: 67.69%, Train_loss: 0.599628, Test_accuracy: 61.79%, Test_loss: 0.660244\n",
      "\n",
      "\n",
      "loss: 0.597803  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.595997 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.659136 \n",
      "\n",
      "Epoch 14: Train_accuracy: 67.99%, Train_loss: 0.595997, Test_accuracy: 62.74%, Test_loss: 0.659136\n",
      "\n",
      "\n",
      "loss: 0.590403  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.587966 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.645338 \n",
      "\n",
      "Epoch 15: Train_accuracy: 68.54%, Train_loss: 0.587966, Test_accuracy: 64.93%, Test_loss: 0.645338\n",
      "\n",
      "\n",
      "loss: 0.591104  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.582616 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.646878 \n",
      "\n",
      "Epoch 16: Train_accuracy: 69.31%, Train_loss: 0.582616, Test_accuracy: 63.27%, Test_loss: 0.646878\n",
      "\n",
      "\n",
      "loss: 0.565819  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.571362 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.644858 \n",
      "\n",
      "Epoch 17: Train_accuracy: 70.14%, Train_loss: 0.571362, Test_accuracy: 63.98%, Test_loss: 0.644858\n",
      "\n",
      "\n",
      "loss: 0.548418  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.566376 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.637878 \n",
      "\n",
      "Epoch 18: Train_accuracy: 70.76%, Train_loss: 0.566376, Test_accuracy: 63.45%, Test_loss: 0.637878\n",
      "\n",
      "\n",
      "loss: 0.564565  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.560082 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.638349 \n",
      "\n",
      "Epoch 19: Train_accuracy: 71.10%, Train_loss: 0.560082, Test_accuracy: 64.45%, Test_loss: 0.638349\n",
      "\n",
      "\n",
      "loss: 0.520795  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.543987 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.635710 \n",
      "\n",
      "Epoch 20: Train_accuracy: 73.49%, Train_loss: 0.543987, Test_accuracy: 64.81%, Test_loss: 0.635710\n",
      "\n",
      "\n",
      "loss: 0.542149  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.537983 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.644668 \n",
      "\n",
      "Epoch 21: Train_accuracy: 73.25%, Train_loss: 0.537983, Test_accuracy: 64.22%, Test_loss: 0.644668\n",
      "\n",
      "\n",
      "loss: 0.538938  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.532488 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.642894 \n",
      "\n",
      "Epoch 22: Train_accuracy: 73.41%, Train_loss: 0.532488, Test_accuracy: 64.93%, Test_loss: 0.642894\n",
      "\n",
      "\n",
      "loss: 0.523173  [ 1024/ 6751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.523060 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.638014 \n",
      "\n",
      "Epoch 23: Train_accuracy: 73.96%, Train_loss: 0.523060, Test_accuracy: 64.81%, Test_loss: 0.638014\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 4\n",
      "loss: 0.693226  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.692345 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.689185 \n",
      "\n",
      "Epoch 1: Train_accuracy: 51.78%, Train_loss: 0.692345, Test_accuracy: 52.96%, Test_loss: 0.689185\n",
      "\n",
      "\n",
      "loss: 0.686874  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.687100 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.686794 \n",
      "\n",
      "Epoch 2: Train_accuracy: 55.40%, Train_loss: 0.687100, Test_accuracy: 55.33%, Test_loss: 0.686794\n",
      "\n",
      "\n",
      "loss: 0.688738  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.681492 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.678061 \n",
      "\n",
      "Epoch 3: Train_accuracy: 57.21%, Train_loss: 0.681492, Test_accuracy: 56.87%, Test_loss: 0.678061\n",
      "\n",
      "\n",
      "loss: 0.668417  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.674190 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.678794 \n",
      "\n",
      "Epoch 4: Train_accuracy: 58.32%, Train_loss: 0.674190, Test_accuracy: 57.05%, Test_loss: 0.678794\n",
      "\n",
      "\n",
      "loss: 0.670289  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.667582 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.671401 \n",
      "\n",
      "Epoch 5: Train_accuracy: 59.13%, Train_loss: 0.667582, Test_accuracy: 57.94%, Test_loss: 0.671401\n",
      "\n",
      "\n",
      "loss: 0.656218  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.662719 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.671910 \n",
      "\n",
      "Epoch 6: Train_accuracy: 59.83%, Train_loss: 0.662719, Test_accuracy: 58.53%, Test_loss: 0.671910\n",
      "\n",
      "\n",
      "loss: 0.665175  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.652367 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.673080 \n",
      "\n",
      "Epoch 7: Train_accuracy: 61.40%, Train_loss: 0.652367, Test_accuracy: 58.77%, Test_loss: 0.673080\n",
      "\n",
      "\n",
      "loss: 0.645447  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.646336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.667157 \n",
      "\n",
      "Epoch 8: Train_accuracy: 62.18%, Train_loss: 0.646336, Test_accuracy: 58.35%, Test_loss: 0.667157\n",
      "\n",
      "\n",
      "loss: 0.629096  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.636917 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.662715 \n",
      "\n",
      "Epoch 9: Train_accuracy: 63.43%, Train_loss: 0.636917, Test_accuracy: 60.84%, Test_loss: 0.662715\n",
      "\n",
      "\n",
      "loss: 0.626328  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.631524 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.652770 \n",
      "\n",
      "Epoch 10: Train_accuracy: 64.29%, Train_loss: 0.631524, Test_accuracy: 61.61%, Test_loss: 0.652770\n",
      "\n",
      "\n",
      "loss: 0.633509  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.624601 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.657577 \n",
      "\n",
      "Epoch 11: Train_accuracy: 65.50%, Train_loss: 0.624601, Test_accuracy: 60.13%, Test_loss: 0.657577\n",
      "\n",
      "\n",
      "loss: 0.610738  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.613116 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.643139 \n",
      "\n",
      "Epoch 12: Train_accuracy: 66.24%, Train_loss: 0.613116, Test_accuracy: 61.85%, Test_loss: 0.643139\n",
      "\n",
      "\n",
      "loss: 0.603316  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.607659 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.658023 \n",
      "\n",
      "Epoch 13: Train_accuracy: 66.11%, Train_loss: 0.607659, Test_accuracy: 61.20%, Test_loss: 0.658023\n",
      "\n",
      "\n",
      "loss: 0.592211  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.599263 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.645421 \n",
      "\n",
      "Epoch 14: Train_accuracy: 67.46%, Train_loss: 0.599263, Test_accuracy: 61.49%, Test_loss: 0.645421\n",
      "\n",
      "\n",
      "loss: 0.594155  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.593700 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.648216 \n",
      "\n",
      "Epoch 15: Train_accuracy: 68.80%, Train_loss: 0.593700, Test_accuracy: 63.51%, Test_loss: 0.648216\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Experiment 5\n",
      "loss: 0.692725  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 52.1%, Avg loss: 0.691789 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.687176 \n",
      "\n",
      "Epoch 1: Train_accuracy: 52.15%, Train_loss: 0.691789, Test_accuracy: 56.31%, Test_loss: 0.687176\n",
      "\n",
      "\n",
      "loss: 0.686461  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686570 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.679928 \n",
      "\n",
      "Epoch 2: Train_accuracy: 55.88%, Train_loss: 0.686570, Test_accuracy: 57.44%, Test_loss: 0.679928\n",
      "\n",
      "\n",
      "loss: 0.682258  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.681059 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.676099 \n",
      "\n",
      "Epoch 3: Train_accuracy: 57.08%, Train_loss: 0.681059, Test_accuracy: 56.49%, Test_loss: 0.676099\n",
      "\n",
      "\n",
      "loss: 0.673263  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.672779 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.673503 \n",
      "\n",
      "Epoch 4: Train_accuracy: 58.16%, Train_loss: 0.672779, Test_accuracy: 57.91%, Test_loss: 0.673503\n",
      "\n",
      "\n",
      "loss: 0.664288  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.670338 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.676666 \n",
      "\n",
      "Epoch 5: Train_accuracy: 58.99%, Train_loss: 0.670338, Test_accuracy: 57.08%, Test_loss: 0.676666\n",
      "\n",
      "\n",
      "loss: 0.665216  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.661585 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.669113 \n",
      "\n",
      "Epoch 6: Train_accuracy: 60.35%, Train_loss: 0.661585, Test_accuracy: 59.04%, Test_loss: 0.669113\n",
      "\n",
      "\n",
      "loss: 0.664460  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.653856 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.670112 \n",
      "\n",
      "Epoch 7: Train_accuracy: 61.63%, Train_loss: 0.653856, Test_accuracy: 58.98%, Test_loss: 0.670112\n",
      "\n",
      "\n",
      "loss: 0.656947  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.647416 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.670294 \n",
      "\n",
      "Epoch 8: Train_accuracy: 61.73%, Train_loss: 0.647416, Test_accuracy: 59.45%, Test_loss: 0.670294\n",
      "\n",
      "\n",
      "loss: 0.627948  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.638659 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.668863 \n",
      "\n",
      "Epoch 9: Train_accuracy: 63.52%, Train_loss: 0.638659, Test_accuracy: 58.62%, Test_loss: 0.668863\n",
      "\n",
      "\n",
      "loss: 0.630158  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.631263 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.668884 \n",
      "\n",
      "Epoch 10: Train_accuracy: 63.88%, Train_loss: 0.631263, Test_accuracy: 61.77%, Test_loss: 0.668884\n",
      "\n",
      "\n",
      "loss: 0.630809  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.629855 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.651620 \n",
      "\n",
      "Epoch 11: Train_accuracy: 63.98%, Train_loss: 0.629855, Test_accuracy: 61.00%, Test_loss: 0.651620\n",
      "\n",
      "\n",
      "loss: 0.617069  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.620905 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.647230 \n",
      "\n",
      "Epoch 12: Train_accuracy: 65.48%, Train_loss: 0.620905, Test_accuracy: 62.18%, Test_loss: 0.647230\n",
      "\n",
      "\n",
      "loss: 0.614215  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.613596 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.654442 \n",
      "\n",
      "Epoch 13: Train_accuracy: 66.37%, Train_loss: 0.613596, Test_accuracy: 61.83%, Test_loss: 0.654442\n",
      "\n",
      "\n",
      "loss: 0.606323  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.604712 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.659661 \n",
      "\n",
      "Epoch 14: Train_accuracy: 67.55%, Train_loss: 0.604712, Test_accuracy: 60.52%, Test_loss: 0.659661\n",
      "\n",
      "\n",
      "loss: 0.596378  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.599060 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.660934 \n",
      "\n",
      "Epoch 15: Train_accuracy: 67.24%, Train_loss: 0.599060, Test_accuracy: 62.24%, Test_loss: 0.660934\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import time\n",
    "def find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes, hyperparameter):\n",
    "    cross_validation_accuracies_dict = {}\n",
    "    cross_validation_times_dict = {}\n",
    "    for batch_size_1 in batch_sizes:\n",
    "        print(batch_size_1)\n",
    "        print(\"----------------------------------\")\n",
    "        print(\"\\n\")\n",
    "        val_classification_accuracy = [] # A list that temporarily stores the number of classification accuracy in the last epoch of every experiment for a particular batch size\n",
    "        elapsed_time_lepoch = [] # A list that temporarily stores the time taken to train the last epoch of every experiment for a particular batch size\n",
    "        for n in range(len(X_train_scaled_dict[batch_size_1])):\n",
    "            print(f\"Experiment {n+1}\")\n",
    "            X_train_batch = X_train_scaled_dict[batch_size_1][n]\n",
    "            X_val_batch = X_val_scaled_dict[batch_size_1][n]\n",
    "            y_train_batch = y_train_dict[batch_size_1][n]\n",
    "            y_val_batch = y_val_dict[batch_size_1][n]\n",
    "            \n",
    "            train_dataloader, val_dataloader = intialise_loaders(X_train_batch, y_train_batch, X_val_batch, y_val_batch, batch_size_1)\n",
    "            \n",
    "            earlyStop = EarlyStopper(patience = es_patience)\n",
    "            model = MLP(no_features = model_no_features, no_hidden = model_no_hidden, no_labels = model_no_labels)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "            loss_fn = nn.BCELoss()\n",
    "            \n",
    "            for t in range(epochs):\n",
    "                start_time = time.time()\n",
    "                train_loss, train_correct = train_loop(train_dataloader, model, loss_fn, optimizer) # In mini batch gradient descent in one epoch the model is trained on all batches\n",
    "                end_time = time.time()\n",
    "                val_loss, val_correct = test_loop(val_dataloader, model, loss_fn)\n",
    "                elapsed_time = end_time - start_time\n",
    "                print(f\"Epoch {t+1}: Train_accuracy: {(100*train_correct):>0.2f}%, Train_loss: {train_loss:>8f}, Test_accuracy: {(100*val_correct):>0.2f}%, Test_loss: {val_loss:>8f}\")\n",
    "                print(\"\\n\")\n",
    "                # print(elapsed_time) Debugging\n",
    "                if earlyStop.early_stop(val_loss): \n",
    "                    print(\"Done!\")\n",
    "                    print(\"\\n\")\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            elapsed_time_last_epoch = elapsed_time\n",
    "            elapsed_time_lepoch.append(elapsed_time_last_epoch)\n",
    "            \n",
    "            val_accuracy_for_last_epoch = val_correct\n",
    "            val_classification_accuracy.append(val_accuracy_for_last_epoch)\n",
    "        \n",
    "        # print(elapsed_time_lepoch) Debugging\n",
    "        average_time_lepoch = np.mean(elapsed_time_lepoch)\n",
    "        #average_time_lepoch = \"{:.2f}\".format(average_time_lepoch_1)\n",
    "        # print(len(val_classification_accuracy)) Debugging\n",
    "        average_classification_accuracy = np.mean(val_classification_accuracy)\n",
    "        #average_classification_accuracy = \"{:.2f}\".format(average_classification_accuracy_1)\n",
    "        # print(average_classification_accuracy) Debugging\n",
    "        cross_validation_accuracies_dict[batch_size_1] = average_classification_accuracy\n",
    "        cross_validation_times_dict[batch_size_1] = average_time_lepoch\n",
    "    return cross_validation_accuracies_dict, cross_validation_times_dict \n",
    "\n",
    "batch_sizes = [128, 256, 512, 1024]\n",
    "cross_validation_accuracies, cross_validation_times = find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes, 'batch_size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c00fa9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validation_accuracies for different batch sizes:\n",
      "{128: 0.6797020426624563, 256: 0.6536333742558791, 512: 0.6394103079866388, 1024: 0.6383438870425361}\n",
      "\n",
      "\n",
      "cross_validation_times for different batch sizes:\n",
      "{128: 0.15106115341186524, 256: 0.11024775505065917, 512: 0.08696017265319825, 1024: 0.0726020336151123}\n"
     ]
    }
   ],
   "source": [
    "print(f\"cross_validation_accuracies for different batch sizes:\\n{cross_validation_accuracies}\")\n",
    "print(\"\\n\")\n",
    "print(f\"cross_validation_times for different batch sizes:\\n{cross_validation_times}\")\n",
    "\n",
    "accuracy_keys_list = list(cross_validation_accuracies.keys())\n",
    "accuracy_values_list = list(cross_validation_accuracies.values())\n",
    "\n",
    "times_keys_list = list(cross_validation_times.keys())\n",
    "times_values_list = list(cross_validation_times.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64384c9c-ddd5-4460-bf37-b9977443a65c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "64384c9c-ddd5-4460-bf37-b9977443a65c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "975e552e751c4efb2cec0eac214f85cd",
     "grade": true,
     "grade_id": "correct_hyperparameter_tuning",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69421943e22521de848bb03a50f57767",
     "grade": false,
     "grade_id": "a2_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "5. Plot scatterplot of mean cross validation accuracies for the different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17599eb29fd6e3a1e2812f0ff7cba983",
     "grade": true,
     "grade_id": "plot",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(accuracy_keys_list, accuracy_values_list, label='Cross validation accuracies accross batch sizes')\n",
    "\n",
    "\n",
    "plt.xlabel('batch_sizes')\n",
    "plt.ylabel('cross validation accuracies')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672cfe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(times_keys_list, times_values_list, label='Average last epoch training time')\n",
    "\n",
    "\n",
    "plt.xlabel('batch_sizes')\n",
    "plt.ylabel('average training time (s)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11e8d298b5774c4044f1c3f950c46214",
     "grade": false,
     "grade_id": "a2_1_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "6. Create a table of time taken to train the network on the last epoch against different batch sizes. Select the optimal batch size and state a reason for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c18e30a9850c282ad725336848222a62",
     "grade": false,
     "grade_id": "times",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Last Epoch Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>0.151061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.110248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>0.086960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.072602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch Size  Last Epoch Time\n",
       "0         128         0.151061\n",
       "1         256         0.110248\n",
       "2         512         0.086960\n",
       "3        1024         0.072602"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Batch Size': times_keys_list, 'Last Epoch Time': times_values_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83d786-706b-46d2-9220-3b09e4c473b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1c83d786-706b-46d2-9220-3b09e4c473b3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2fc4a52c2a0af7ea586ea85cec9b3e9",
     "grade": true,
     "grade_id": "correct_times",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46dfd1c-1d3c-46e4-98d6-21c2672ad31b",
   "metadata": {
    "deletable": false,
    "id": "d46dfd1c-1d3c-46e4-98d6-21c2672ad31b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38690f32ec506325fc73c8353b77d041",
     "grade": false,
     "grade_id": "batch_size",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "optimal_batch_size = 128\n",
    "reason = \"The model trained with training data with batch size of 128 has the highest mean cross validation accuracy. At the same time, training the model with this batch size takes higher time because of fewer parallel operations compared to those in higher batch sizes but this is expected.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ff7b5-6a77-47d4-941e-37bc495b6558",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "096ff7b5-6a77-47d4-941e-37bc495b6558",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f695b961ed43ec6a31b7647e078fd8d6",
     "grade": true,
     "grade_id": "correct_batch_size",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
