{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnRX6LUnqBpw"
   },
   "source": [
    "CS4001/4042 Assignment 1\n",
    "---\n",
    "Part B, Q1 (15 marks)\n",
    "---\n",
    "\n",
    "Real world datasets often have a mix of numeric and categorical features – this dataset is one example. To build models on such data, categorical features have to be encoded or embedded.\n",
    "\n",
    "PyTorch Tabular is a library that makes it very convenient to build neural networks for tabular data. It is built on top of PyTorch Lightning, which abstracts away boilerplate model training code and makes it easy to integrate other tools, e.g. TensorBoard for experiment tracking.\n",
    "\n",
    "For questions B1 and B2, the following features should be used:   \n",
    "- **Numeric / Continuous** features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
    "- **Categorical** features: month, town, flat_model_type, storey_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jA67PbIY3PnH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pytorch_tabular[extra]\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_tabular[extra]\n",
    "# ! pip install numpy\n",
    "# ! pip install pandas\n",
    "# ! pip install torch\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Jr6P3U7w3NVl"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGyEWcVlqKTz"
   },
   "source": [
    "> Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from year 2020 and before as training data, and year 2021 as test data (validation set is not required).\n",
    "**Do not** use data from year 2022 and year 2023.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hoCPcOWupw5Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (159553, 14)\n",
      "Year range: 2017 to 2023\n",
      "Train set (≤2020): 87370 records\n",
      "Test set (2021): 29057 records\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Year range: {df['year'].min()} to {df['year'].max()}\")\n",
    "\n",
    "# Split data by year\n",
    "train_df = df[df['year'] <= 2020]\n",
    "test_df_2021 = df[df['year'] == 2021]\n",
    "\n",
    "print(f\"Train set (≤2020): {train_df.shape[0]} records\")\n",
    "print(f\"Test set (2021): {test_df_2021.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sebMgSuzqPe7"
   },
   "source": [
    "> Refer to the documentation of **PyTorch Tabular** and perform the following tasks: https://pytorch-tabular.readthedocs.io/en/latest/#usage\n",
    "- Use **[DataConfig](https://pytorch-tabular.readthedocs.io/en/latest/data/)** to define the target variable, as well as the names of the continuous and categorical variables.\n",
    "- Use **[TrainerConfig](https://pytorch-tabular.readthedocs.io/en/latest/training/)** to automatically tune the learning rate. Set batch_size to be 1024 and set max_epoch as 50.\n",
    "- Use **[CategoryEmbeddingModelConfig](https://pytorch-tabular.readthedocs.io/en/latest/models/#category-embedding-model)** to create a feedforward neural network with 1 hidden layer containing 50 neurons.\n",
    "- Use **[OptimizerConfig](https://pytorch-tabular.readthedocs.io/en/latest/optimizer/)** to choose Adam optimiser. There is no need to set the learning rate (since it will be tuned automatically) nor scheduler.\n",
    "- Use **[TabularModel](https://pytorch-tabular.readthedocs.io/en/latest/tabular_model/)** to initialise the model and put all the configs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZZWAYdNhqPzh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">494</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:03\u001b[0m,\u001b[1;36m494\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the PyTorch Tabular model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">517</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:03\u001b[0m,\u001b[1;36m517\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">543</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:03\u001b[0m,\u001b[1;36m543\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">621</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: CategoryEmbeddingModel \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:03\u001b[0m,\u001b[1;36m621\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: CategoryEmbeddingModel \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">642</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:03\u001b[0m,\u001b[1;36m642\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:03</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">658</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:03\u001b[0m,\u001b[1;36m658\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/saved_models exists and is not empty.\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33009d731adc4c3fa74637fcde528fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.5754399373371567\n",
      "Restoring states from the checkpoint path at /Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.lr_find_81c11c4f-b60e-4045-b005-87754d39b359.ckpt\n",
      "Restored all states from the checkpoint at /Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.lr_find_81c11c4f-b60e-4045-b005-87754d39b359.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">325</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5754399373371567</span>. For plot  \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:06\u001b[0m,\u001b[1;36m325\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.5754399373371567\u001b[0m. For plot  \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">327</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:06\u001b[0m,\u001b[1;36m327\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer          │  1.6 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ head             │ LinearHead                │     51 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ MSELoss                   │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer          │  1.6 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ head             │ LinearHead                │     51 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ MSELoss                   │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.6 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.6 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 16                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.6 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.6 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 16                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b74f5c56cf942dc8c3139a6e1747084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">699</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:26\u001b[0m,\u001b[1;36m699\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:22:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">700</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:22:26\u001b[0m,\u001b[1;36m700\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL builtins.dict was not an allowed global by default. Please use `torch.serialization.add_safe_globals([dict])` or the `torch.serialization.safe_globals([dict])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m tabular_model \u001b[38;5;241m=\u001b[39m TabularModel(\n\u001b[1;32m     30\u001b[0m     data_config\u001b[38;5;241m=\u001b[39mdata_config,\n\u001b[1;32m     31\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m     32\u001b[0m     optimizer_config\u001b[38;5;241m=\u001b[39moptimizer_config,\n\u001b[1;32m     33\u001b[0m     trainer_config\u001b[38;5;241m=\u001b[39mtrainer_config,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the PyTorch Tabular model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtabular_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Evaluate the model on 2021 test data\u001b[39;00m\n\u001b[1;32m     40\u001b[0m results \u001b[38;5;241m=\u001b[39m tabular_model\u001b[38;5;241m.\u001b[39mevaluate(test_df_2021)\n",
      "File \u001b[0;32m~/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/tabular_model.py:806\u001b[0m, in \u001b[0;36mTabularModel.fit\u001b[0;34m(self, train, validation, loss, metrics, metrics_prob_inputs, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, seed, callbacks, datamodule, cache_data, handle_oom)\u001b[0m\n\u001b[1;32m    792\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain data and datamodule is provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Ignoring the train data and using the datamodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Set either one of them to None to avoid this warning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m         )\n\u001b[1;32m    797\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_model(\n\u001b[1;32m    798\u001b[0m     datamodule,\n\u001b[1;32m    799\u001b[0m     loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    803\u001b[0m     optimizer_params \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    804\u001b[0m )\n\u001b[0;32m--> 806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_oom\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/tabular_model.py:691\u001b[0m, in \u001b[0;36mTabularModel.train\u001b[0;34m(self, model, datamodule, callbacks, max_epochs, min_epochs, handle_oom)\u001b[0m\n\u001b[1;32m    689\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mload_best:\n\u001b[0;32m--> 691\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n",
      "File \u001b[0;32m~/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/tabular_model.py:1534\u001b[0m, in \u001b[0;36mTabularModel.load_best_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m   1533\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1534\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_tabular/utils/python_utils.py:85\u001b[0m, in \u001b[0;36mpl_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     83\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/torch/serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1464\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1472\u001b[0m             opened_zipfile,\n\u001b[1;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1477\u001b[0m         )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL builtins.dict was not an allowed global by default. Please use `torch.serialization.add_safe_globals([dict])` or the `torch.serialization.safe_globals([dict])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "# Define features as specified in the assignment\n",
    "categorical_features = ['month', 'town', 'flat_model_type', 'storey_range']\n",
    "continuous_features = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', \n",
    "                      'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "target = ['resale_price']\n",
    "\n",
    "# Configure the model\n",
    "data_config = DataConfig(\n",
    "    target=target,\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=categorical_features,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,  # Automatically tune learning rate\n",
    "    batch_size=1024,\n",
    "    max_epochs=50,\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"regression\",\n",
    "    layers=\"50\"\n",
    ")\n",
    "\n",
    "# Initialize and train the model\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "print(\"Training the PyTorch Tabular model...\")\n",
    "tabular_model.fit(train=train_df)\n",
    "\n",
    "# Evaluate the model on 2021 test data\n",
    "results = tabular_model.evaluate(test_df_2021)\n",
    "pred_df = tabular_model.predict(test_df_2021)\n",
    "# test_preds = pred_df[target + \"_prediction\"].values\n",
    "# test_actuals = test_df_2021[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2UXPKq0qWQG"
   },
   "source": [
    "> Report the test RMSE error and the test R2 value that you obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zmE9Bc7Nqadi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B1 Results on 2021 Test Set:\n",
      "RMSE: $76635.10\n",
      "R² Score: 0.7780\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(test_actuals, test_preds))\n",
    "r2 = r2_score(test_actuals, test_preds)\n",
    "\n",
    "print(f\"\\nB1 Results on 2021 Test Set:\")\n",
    "print(f\"RMSE: ${rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEJhRU18qX22"
   },
   "source": [
    "> Print out the corresponding rows in the dataframe for the top 25 test samples with the largest errors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5ma5K9vKqZEq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 25 samples with largest prediction errors:\n",
      "        year  month          town        flat_model_type  floor_area_sqm  \\\n",
      "92405   2021     11   BUKIT MERAH       3 ROOM, Standard            88.0   \n",
      "90608   2021     12        BISHAN           5 ROOM, DBSS           120.0   \n",
      "90957   2021      6   BUKIT BATOK   EXECUTIVE, Apartment           144.0   \n",
      "92442   2021     11   BUKIT MERAH       5 ROOM, Improved           113.0   \n",
      "112128  2021     12      TAMPINES  EXECUTIVE, Maisonette           148.0   \n",
      "90521   2021     10        BISHAN       5 ROOM, Improved           121.0   \n",
      "90432   2021      8        BISHAN           5 ROOM, DBSS           120.0   \n",
      "90483   2021      9        BISHAN           5 ROOM, DBSS           120.0   \n",
      "98379   2021     12       HOUGANG   EXECUTIVE, Apartment           142.0   \n",
      "105702  2021      6    QUEENSTOWN   EXECUTIVE, Apartment           148.0   \n",
      "92533   2021     12   BUKIT MERAH       5 ROOM, Improved           115.0   \n",
      "91871   2021      6   BUKIT MERAH       3 ROOM, Standard            88.0   \n",
      "93670   2021     12   BUKIT TIMAH  EXECUTIVE, Maisonette           154.0   \n",
      "88081   2021      8    ANG MO KIO       5 ROOM, Improved           121.0   \n",
      "90523   2021     10        BISHAN           5 ROOM, DBSS           120.0   \n",
      "92340   2021     10   BUKIT MERAH       5 ROOM, Improved           114.0   \n",
      "93825   2021      8  CENTRAL AREA  5 ROOM, Adjoined flat           118.0   \n",
      "90607   2021     12        BISHAN           5 ROOM, DBSS           120.0   \n",
      "92226   2021      9   BUKIT MERAH       5 ROOM, Improved           113.0   \n",
      "92504   2021     12   BUKIT MERAH       3 ROOM, Standard            88.0   \n",
      "90382   2021      7        BISHAN       5 ROOM, Improved           121.0   \n",
      "109937  2021     11     SERANGOON  EXECUTIVE, Maisonette           150.0   \n",
      "90253   2021      4        BISHAN           5 ROOM, DBSS           120.0   \n",
      "92299   2021     10   BUKIT MERAH       3 ROOM, Standard            88.0   \n",
      "90431   2021      8        BISHAN           5 ROOM, DBSS           120.0   \n",
      "\n",
      "        resale_price     predicted         error  \n",
      "92405       780000.0  389912.40625  390087.59375  \n",
      "90608      1360000.0  987042.75000  372957.25000  \n",
      "90957       968000.0  626774.18750  341225.81250  \n",
      "92442      1165000.0  839517.93750  325482.06250  \n",
      "112128      998000.0  675974.00000  322026.00000  \n",
      "90521       988000.0  667874.31250  320125.68750  \n",
      "90432      1280000.0  960885.31250  319114.68750  \n",
      "90483      1295000.0  983944.06250  311055.93750  \n",
      "98379       873000.0  562176.43750  310823.56250  \n",
      "105702     1235000.0  925239.56250  309760.43750  \n",
      "92533      1130000.0  821663.43750  308336.56250  \n",
      "91871       680888.0  378729.87500  302158.12500  \n",
      "93670      1238000.0  936202.00000  301798.00000  \n",
      "88081      1100000.0  798295.81250  301704.18750  \n",
      "90523      1260000.0  959246.93750  300753.06250  \n",
      "92340      1245000.0  945075.06250  299924.93750  \n",
      "93825       938000.0  638492.43750  299507.56250  \n",
      "90607      1208000.0  912644.93750  295355.06250  \n",
      "92226      1220000.0  925335.43750  294664.56250  \n",
      "92504       695000.0  402773.18750  292226.81250  \n",
      "90382       945000.0  653562.93750  291437.06250  \n",
      "109937      935000.0  644543.31250  290456.68750  \n",
      "90253      1250000.0  960926.25000  289073.75000  \n",
      "92299       690000.0  402693.06250  287306.93750  \n",
      "90431      1240000.0  956885.31250  283114.68750  \n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "# Calculate errors for each test sample\n",
    "test_df_2021['predicted'] = test_preds\n",
    "test_df_2021['error'] = np.abs(test_df_2021['predicted'] - test_df_2021[target])\n",
    "\n",
    "# Show top 25 samples with largest errors\n",
    "top_25_errors = test_df_2021.sort_values(by='error', ascending=False).head(25)\n",
    "print(\"\\nTop 25 samples with largest prediction errors:\")\n",
    "print(top_25_errors[['year', 'month', 'town', 'flat_model_type', 'floor_area_sqm', 'resale_price', 'predicted', 'error']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q2 (10 marks)\n",
    "---\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Downloading pytorch_widedeep-1.6.5-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in ./.venv/lib/python3.10/site-packages (from pytorch-widedeep) (2.2.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.6 in ./.venv/lib/python3.10/site-packages (from pytorch-widedeep) (1.26.4)\n",
      "Collecting scipy<=1.12.0,>=1.7.3 (from pytorch-widedeep)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (112 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./.venv/lib/python3.10/site-packages (from pytorch-widedeep) (1.6.1)\n",
      "Collecting gensim (from pytorch-widedeep)\n",
      "  Downloading gensim-4.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.2 kB)\n",
      "Collecting spacy (from pytorch-widedeep)\n",
      "  Downloading spacy-3.8.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting opencv-contrib-python>=4.9.0.80 (from pytorch-widedeep)\n",
      "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from pytorch-widedeep) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.10/site-packages (from pytorch-widedeep) (2.6.0)\n",
      "Collecting torchvision>=0.15.0 (from pytorch-widedeep)\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: einops in ./.venv/lib/python3.10/site-packages (from pytorch-widedeep) (0.7.0)\n",
      "Collecting wrapt (from pytorch-widedeep)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: torchmetrics>=1.3.1 in ./.venv/lib/python3.10/site-packages (from pytorch-widedeep) (1.5.2)\n",
      "Collecting pyarrow>=15.0.0 (from pytorch-widedeep)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting fastparquet>=2024.2.0 (from pytorch-widedeep)\n",
      "  Downloading fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting transformers>=4.37.0 (from pytorch-widedeep)\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting sentence-transformers>=2.3.0 (from pytorch-widedeep)\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from pytorch-widedeep)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting cramjam>=2.3 (from fastparquet>=2024.2.0->pytorch-widedeep)\n",
      "  Downloading cramjam-2.9.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from fastparquet>=2024.2.0->pytorch-widedeep) (2025.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from fastparquet>=2024.2.0->pytorch-widedeep) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-widedeep) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.6.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers>=2.3.0->pytorch-widedeep)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from sentence-transformers>=2.3.0->pytorch-widedeep) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->pytorch-widedeep) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->pytorch-widedeep) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->pytorch-widedeep) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->pytorch-widedeep) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->pytorch-widedeep) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->pytorch-widedeep) (1.3.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in ./.venv/lib/python3.10/site-packages (from torchmetrics>=1.3.1->pytorch-widedeep) (0.14.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers>=4.37.0->pytorch-widedeep) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.37.0->pytorch-widedeep)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers>=4.37.0->pytorch-widedeep) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.37.0->pytorch-widedeep)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.37.0->pytorch-widedeep)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim->pytorch-widedeep)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->pytorch-widedeep)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading murmurhash-1.0.12-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->pytorch-widedeep)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->pytorch-widedeep)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy->pytorch-widedeep)\n",
      "  Downloading thinc-8.3.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->pytorch-widedeep)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->pytorch-widedeep)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->pytorch-widedeep)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy->pytorch-widedeep)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy->pytorch-widedeep) (75.6.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep) (2025.1.31)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy->pytorch-widedeep)\n",
      "  Downloading blis-1.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy->pytorch-widedeep)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->pytorch-widedeep)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->pytorch-widedeep) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (0.1.2)\n",
      "Downloading pytorch_widedeep-1.6.5-py3-none-any.whl (22.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastparquet-2024.11.0-cp310-cp310-macosx_11_0_arm64.whl (684 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.1/684.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (46.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gensim-4.3.3-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.8.4-cp310-cp310-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cramjam-2.9.1-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.4/634.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp310-cp310-macosx_11_0_arm64.whl (779 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.4/779.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "Installing collected packages: sentencepiece, cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, scipy, safetensors, regex, pydantic-core, pyarrow, opencv-contrib-python, murmurhash, marisa-trie, cramjam, cloudpathlib, click, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, language-data, huggingface-hub, typer, torchvision, tokenizers, langcodes, gensim, fastparquet, confection, weasel, transformers, thinc, spacy, sentence-transformers, pytorch-widedeep\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 click-8.1.8 cloudpathlib-0.21.0 confection-0.1.5 cramjam-2.9.1 cymem-2.0.11 fastparquet-2024.11.0 gensim-4.3.3 huggingface-hub-0.29.3 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 opencv-contrib-python-4.11.0.86 preshed-3.0.9 pyarrow-19.0.1 pydantic-2.10.6 pydantic-core-2.27.2 pytorch-widedeep-1.6.5 regex-2024.11.6 safetensors-0.5.3 scipy-1.12.0 sentence-transformers-3.4.1 sentencepiece-0.2.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 tokenizers-0.21.1 torchvision-0.21.0 transformers-4.49.0 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "! pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data（validation set is not required here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part B, Question 2: PyTorch-WideDeep Implementation\n",
      "Train set (≤2020): 87370 records\n",
      "Test set (≥2021): 72183 records\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "print(\"\\nPart B, Question 2: PyTorch-WideDeep Implementation\")\n",
    "\n",
    "# For B2, test set includes 2021 and after\n",
    "test_df_b2 = df[df['year'] >= 2021].copy()\n",
    "print(f\"Train set (≤2020): {train_df.shape[0]} records\")\n",
    "print(f\"Test set (≥2021): {test_df_b2.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 hidden layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 60 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryansethi/Documents/Personal/Neural-Networks-Assignment-1/.venv/lib/python3.10/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:299: DeprecationWarning: 'scale' and 'already_standard' will be deprecated in the next release. Please use 'cols_to_scale' instead\n",
      "  self._check_inputs(cat_embed_cols)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TabPreprocessor' object has no attribute 'cat_embed_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m X_tab_test \u001b[38;5;241m=\u001b[39m tab_preprocessor\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create the TabMlp model with 2 hidden layers (200, 100)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m tab_mlp \u001b[38;5;241m=\u001b[39m TabMlp(\n\u001b[1;32m     19\u001b[0m     column_idx\u001b[38;5;241m=\u001b[39mtab_preprocessor\u001b[38;5;241m.\u001b[39mcolumn_idx,\n\u001b[1;32m     20\u001b[0m     mlp_hidden_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m     21\u001b[0m     mlp_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 22\u001b[0m     embed_input\u001b[38;5;241m=\u001b[39m\u001b[43mtab_preprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_embed_input\u001b[49m,\n\u001b[1;32m     23\u001b[0m     continuous_cols\u001b[38;5;241m=\u001b[39mcontinuous_features\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Combine the components\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m WideDeep(deeptabular\u001b[38;5;241m=\u001b[39mtab_mlp)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TabPreprocessor' object has no attribute 'cat_embed_input'"
     ]
    }
   ],
   "source": [
    "# Prepare data for TabPreprocessor\n",
    "X_train = train_df.copy()\n",
    "y_train = X_train.pop(target)\n",
    "X_test = test_df_b2.copy()\n",
    "y_test = X_test.pop(target)\n",
    "\n",
    "# Preprocess the tabular data\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    categorical_cols=categorical_features,\n",
    "    continuous_cols=continuous_features,\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "X_tab_train = tab_preprocessor.fit_transform(X_train)\n",
    "X_tab_test = tab_preprocessor.transform(X_test)\n",
    "\n",
    "# Create the TabMlp model with 2 hidden layers (200, 100)\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    mlp_hidden_dims=[200, 100],\n",
    "    mlp_activation=\"relu\",\n",
    "    embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_features\n",
    ")\n",
    "\n",
    "# Combine the components\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    objective=\"regression\",\n",
    "    optimizers=\"Adam\",\n",
    "    lr=0.001,\n",
    "    metrics=[R2Score],\n",
    "    callbacks=None,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the PyTorch-WideDeep model...\")\n",
    "trainer.fit(\n",
    "    X_tab=X_tab_train,\n",
    "    target=y_train,\n",
    "    n_epochs=60,\n",
    "    batch_size=64,\n",
    "    val_split=0.1,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "# Evaluate on test set\n",
    "preds = trainer.predict(X_tab=X_tab_test)\n",
    "rmse_b2 = np.sqrt(mean_squared_error(y_test, preds))\n",
    "r2_b2 = r2_score(y_test, preds)\n",
    "\n",
    "print(f\"\\nB2 Results on Test Set (≥2021):\")\n",
    "print(f\"RMSE: ${rmse_b2:.2f}\")\n",
    "print(f\"R² Score: {r2_b2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q3 (10 marks)\n",
    "---\n",
    "Besides ensuring that your neural network performs well, it is important to be able to explain the model’s decision. **Captum** is a very handy library that helps you to do so for PyTorch models.\n",
    "\n",
    "Many model explainability algorithms for deep learning models are available in Captum. These algorithms are often used to generate an attribution score for each feature. Features with larger scores are more ‘important’ and some algorithms also provide information about directionality (i.e. a feature with very negative attribution scores means the larger the value of that feature, the lower the value of the output).\n",
    "\n",
    "In general, these algorithms can be grouped into two paradigms:\n",
    "- **perturbation based approaches** (e.g. Feature Ablation)\n",
    "- **gradient / backpropagation based approaches** (e.g. Saliency)\n",
    "\n",
    "The former adopts a brute-force approach of removing / permuting features one by one and does not scale up well. The latter depends on gradients and they can be computed relatively quickly. But unlike how backpropagation computes gradients with respect to weights, gradients here are computed **with respect to the input**. This gives us a sense of how much a change in the input affects the model’s outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, GradientShap, FeatureAblation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, use the train set (year 2020 and before) and test set (year 2021) following the splits in Question B1 (validation set is not required here). To keep things simple, we will **limit our analysis to numeric / continuous features only**. Drop all categorical features from the dataframes. Standardise the features via **StandardScaler** (fit to training set, then transform all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Follow this tutorial to generate the plot from various model explainability algorithms (https://captum.ai/tutorials/House_Prices_Regression_Interpret).\n",
    "Specifically, make the following changes:\n",
    "- Use a feedforward neural network with 3 hidden layers, each having 5 neurons. Train using Adam optimiser with learning rate of 0.001.\n",
    "- Use Input x Gradients, Integrated Gradients, DeepLift, GradientSHAP, Feature Ablation. To avoid long running time, you can limit the analysis to the first 1000 samples in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read the following [descriptions](https://captum.ai/docs/attribution_algorithms) and [comparisons](https://captum.ai/docs/algorithms_comparison_matrix) in Captum to build up your understanding of the difference of various explainability algorithms. Based on your plot, identify the three most important features for regression. Explain how each of these features influences the regression outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q4 (10 marks)\n",
    "---\n",
    "\n",
    "Model degradation is a common issue faced when deploying machine learning models (including neural networks) in the real world. New data points could exhibit a different pattern from older data points due to factors such as changes in government policy or market sentiments. For instance, housing prices in Singapore have been increasing and the Singapore government has introduced 3 rounds of cooling measures over the past years (16 December 2021, 30 September 2022, 27 April 2023).\n",
    "\n",
    "In such situations, the distribution of the new data points could differ from the original data distribution which the models were trained on. Recall that machine learning models often work with the assumption that the test distribution should be similar to train distribution. When this assumption is violated, model performance will be adversely impacted.  In the last part of this assignment, we will investigate to what extent model degradation has occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install alibi-detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import TabularDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2022 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2023 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Did model degradation occur for the deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model degradation could be caused by [various data distribution shifts](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html#data-shift-types): covariate shift (features), label shift and/or concept drift (altered relationship between features and labels).\n",
    "There are various conflicting terminologies in the [literature](https://www.sciencedirect.com/science/article/pii/S0950705122002854#tbl1). Let’s stick to this reference for this assignment.\n",
    "\n",
    "> Using the **Alibi Detect** library, apply the **TabularDrift** function with the training data (year 2020 and before) used as the reference and **detect which features have drifted** in the 2023 test dataset. Before running the statistical tests, ensure you **sample 1000 data points** each from the train and test data. Do not use the whole train/test data. (Hint: use this example as a guide https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_chi2ks_adult.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assuming that the flurry of housing measures have made an impact on the relationship between all the features and resale_price (i.e. P(Y|X) changes), which type of data distribution shift possibly led to model degradation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From your analysis via TabularDrift, which features contribute to this shift?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suggest 1 way to address model degradation and implement it, showing improved test R2 for year 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOG8ZhA98h3O6fnefkjOU9w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
